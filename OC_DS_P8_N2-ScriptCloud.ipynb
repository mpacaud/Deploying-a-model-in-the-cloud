{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aad6b322",
   "metadata": {},
   "source": [
    "# Déployer un modèle dans le cloud\n",
    "# Notebook 2 - Déploiement et mise en place d'un cluster de serveurs dans le cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907ee006",
   "metadata": {},
   "source": [
    "# Objectif du notebook\n",
    "\n",
    "Dans le notebook précédent nous avons vérifié que notre solution fonctionne, il est temps de la <u>déployer à plus grande échelle</u> sur un vrai cluster de machines dans le cloud.\n",
    "\n",
    "Plusieurs questions se posent :\n",
    " 1. Quel prestataire de Cloud choisir ?\n",
    " 2. Quelles solutions de ce prestataire adopter ?\n",
    " 3. Où stocker nos données ?\n",
    " 4. Comment configurer nos outils dans ce nouvel environnement ?\n",
    " \n",
    "*NB : Ce travail est réalisé dans un environnement virtuel linux kubuntu, les commandes décrites ci-dessous sont réalisées dans cet environnement.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a262cb98",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Déployer-un-modèle-dans-le-cloud\" data-toc-modified-id=\"Déployer-un-modèle-dans-le-cloud-1\">Déployer un modèle dans le cloud</a></span></li><li><span><a href=\"#Notebook-2---Déploiement-et-mise-en-place-d'un-cluster-de-serveurs-dans-le-cloud\" data-toc-modified-id=\"Notebook-2---Déploiement-et-mise-en-place-d'un-cluster-de-serveurs-dans-le-cloud-2\">Notebook 2 - Déploiement et mise en place d'un cluster de serveurs dans le cloud</a></span></li><li><span><a href=\"#Objectif-du-notebook\" data-toc-modified-id=\"Objectif-du-notebook-3\">Objectif du notebook</a></span></li><li><span><a href=\"#1.-Configuration-cloud\" data-toc-modified-id=\"1.-Configuration-cloud-4\">1. Configuration cloud</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1-Choix-du-prestataire-cloud-:-AWS\" data-toc-modified-id=\"1.1-Choix-du-prestataire-cloud-:-AWS-4.1\">1.1 Choix du prestataire cloud : AWS</a></span></li><li><span><a href=\"#1.2-Choix-de-la-solution-technique-:-EMR\" data-toc-modified-id=\"1.2-Choix-de-la-solution-technique-:-EMR-4.2\">1.2 Choix de la solution technique : EMR</a></span></li><li><span><a href=\"#1.3-Choix-de-la-solution-de-stockage-des-données-:-Amazon-S3\" data-toc-modified-id=\"1.3-Choix-de-la-solution-de-stockage-des-données-:-Amazon-S3-4.3\">1.3 Choix de la solution de stockage des données : Amazon S3</a></span></li><li><span><a href=\"#1.4-Configuration-de-l'environnement-de-travail\" data-toc-modified-id=\"1.4-Configuration-de-l'environnement-de-travail-4.4\">1.4 Configuration de l'environnement de travail</a></span></li><li><span><a href=\"#1.5-Upload-de-nos-données-sur-S3\" data-toc-modified-id=\"1.5-Upload-de-nos-données-sur-S3-4.5\">1.5 Upload de nos données sur S3</a></span></li><li><span><a href=\"#1.6-Configuration-du-serveur-EMR\" data-toc-modified-id=\"1.6-Configuration-du-serveur-EMR-4.6\">1.6 Configuration du serveur EMR</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.6.1-Étape-1-:-Logiciels-et-étapes\" data-toc-modified-id=\"1.6.1-Étape-1-:-Logiciels-et-étapes-4.6.1\">1.6.1 Étape 1 : Logiciels et étapes</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.6.1.1-Configuration-des-logiciels\" data-toc-modified-id=\"1.6.1.1-Configuration-des-logiciels-4.6.1.1\">1.6.1.1 Configuration des logiciels</a></span></li><li><span><a href=\"#1.6.1.2-Modifier-les-paramètres-du-logiciel\" data-toc-modified-id=\"1.6.1.2-Modifier-les-paramètres-du-logiciel-4.6.1.2\">1.6.1.2 Modifier les paramètres du logiciel</a></span></li></ul></li><li><span><a href=\"#1.6.2-Étape-2-:-Matériel\" data-toc-modified-id=\"1.6.2-Étape-2-:-Matériel-4.6.2\">1.6.2 Étape 2 : Matériel</a></span></li><li><span><a href=\"#1.6.3-Étape-3-:-Paramètres-généraux-de-cluster\" data-toc-modified-id=\"1.6.3-Étape-3-:-Paramètres-généraux-de-cluster-4.6.3\">1.6.3 Étape 3 : Paramètres généraux de cluster</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.6.3.1-Résiliation-du-cluster\" data-toc-modified-id=\"1.6.3.1-Résiliation-du-cluster-4.6.3.1\">1.6.3.1 Résiliation du cluster</a></span></li><li><span><a href=\"#1.6.3.2-Actions-d'amorçage\" data-toc-modified-id=\"1.6.3.2-Actions-d'amorçage-4.6.3.2\">1.6.3.2 Actions d'amorçage</a></span></li><li><span><a href=\"#1.6.3.3-Journalisation\" data-toc-modified-id=\"1.6.3.3-Journalisation-4.6.3.3\">1.6.3.3 Journalisation</a></span></li></ul></li><li><span><a href=\"#1.6.4-Étape-4-:-Sécurité\" data-toc-modified-id=\"1.6.4-Étape-4-:-Sécurité-4.6.4\">1.6.4 Étape 4 : Sécurité</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.6.4.1-Connection-SSH\" data-toc-modified-id=\"1.6.4.1-Connection-SSH-4.6.4.1\">1.6.4.1 Connection SSH</a></span></li><li><span><a href=\"#1.6.4.2-Configuration-des-rôles-IAM\" data-toc-modified-id=\"1.6.4.2-Configuration-des-rôles-IAM-4.6.4.2\">1.6.4.2 Configuration des rôles IAM</a></span></li></ul></li></ul></li><li><span><a href=\"#1.7-Instanciation-du-serveur\" data-toc-modified-id=\"1.7-Instanciation-du-serveur-4.7\">1.7 Instanciation du serveur</a></span></li><li><span><a href=\"#1.8-Création-du-tunnel-SSH-à-l'instance-EC2-(Maître)\" data-toc-modified-id=\"1.8-Création-du-tunnel-SSH-à-l'instance-EC2-(Maître)-4.8\">1.8 Création du tunnel SSH à l'instance EC2 (Maître)</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.8.1-Création-des-autorisations-sur-les-connexions-entrantes\" data-toc-modified-id=\"1.8.1-Création-des-autorisations-sur-les-connexions-entrantes-4.8.1\">1.8.1 Création des autorisations sur les connexions entrantes</a></span></li><li><span><a href=\"#1.8.2-Création-du-tunnel-ssh-vers-le-Driver\" data-toc-modified-id=\"1.8.2-Création-du-tunnel-ssh-vers-le-Driver-4.8.2\">1.8.2 Création du tunnel ssh vers le Driver</a></span></li><li><span><a href=\"#1.8.3-Configuration-de-FoxyProxy\" data-toc-modified-id=\"1.8.3-Configuration-de-FoxyProxy-4.8.3\">1.8.3 Configuration de FoxyProxy</a></span></li><li><span><a href=\"#1.8.4-Accès-aux-applications-du-serveur-EMR-via-le-tunnel-ssh\" data-toc-modified-id=\"1.8.4-Accès-aux-applications-du-serveur-EMR-via-le-tunnel-ssh-4.8.4\">1.8.4 Accès aux applications du serveur EMR via le tunnel ssh</a></span></li></ul></li><li><span><a href=\"#1.9-Connexion-au-notebook-JupyterHub\" data-toc-modified-id=\"1.9-Connexion-au-notebook-JupyterHub-4.9\">1.9 Connexion au notebook JupyterHub</a></span></li></ul></li><li><span><a href=\"#2.-Exécution-du-code\" data-toc-modified-id=\"2.-Exécution-du-code-5\">2. Exécution du code</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.1-Démarrage-de-la-session-Spark\" data-toc-modified-id=\"2.1-Démarrage-de-la-session-Spark-5.1\">2.1 Démarrage de la session Spark</a></span></li><li><span><a href=\"#2.2-Installation-des-paquets\" data-toc-modified-id=\"2.2-Installation-des-paquets-5.2\">2.2 Installation des paquets</a></span></li><li><span><a href=\"#2.3-Configuration-graphique-du-notebook\" data-toc-modified-id=\"2.3-Configuration-graphique-du-notebook-5.3\">2.3 Configuration graphique du notebook</a></span></li><li><span><a href=\"#2.4-Import-des-librairies\" data-toc-modified-id=\"2.4-Import-des-librairies-5.4\">2.4 Import des librairies</a></span></li><li><span><a href=\"#2.5-Définition-des-PATH-pour-charger-les-images-et-enregistrer-les-résultats\" data-toc-modified-id=\"2.5-Définition-des-PATH-pour-charger-les-images-et-enregistrer-les-résultats-5.5\">2.5 Définition des PATH pour charger les images et enregistrer les résultats</a></span></li><li><span><a href=\"#2.6-Traitement-des-données\" data-toc-modified-id=\"2.6-Traitement-des-données-5.6\">2.6 Traitement des données</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.6.1-Chargement-des-données\" data-toc-modified-id=\"2.6.1-Chargement-des-données-5.6.1\">2.6.1 Chargement des données</a></span></li><li><span><a href=\"#2.6.2-Préparation-du-modèle\" data-toc-modified-id=\"2.6.2-Préparation-du-modèle-5.6.2\">2.6.2 Préparation du modèle</a></span></li><li><span><a href=\"#2.6.3-Définition-du-processus-de-chargement-des-images-et-application-de-leur-featurisation-à-travers-l'utilisation-de-pandas-UDF\" data-toc-modified-id=\"2.6.3-Définition-du-processus-de-chargement-des-images-et-application-de-leur-featurisation-à-travers-l'utilisation-de-pandas-UDF-5.6.3\">2.6.3 Définition du processus de chargement des images et application de leur featurisation à travers l'utilisation de pandas UDF</a></span></li><li><span><a href=\"#2.6.4-Exécutions-des-actions-d'extractions-de-features\" data-toc-modified-id=\"2.6.4-Exécutions-des-actions-d'extractions-de-features-5.6.4\">2.6.4 Exécutions des actions d'extractions de features</a></span></li></ul></li><li><span><a href=\"#2.7-Ajout-d'une-ACP-pour-optimiser-les-performances\" data-toc-modified-id=\"2.7-Ajout-d'une-ACP-pour-optimiser-les-performances-5.7\">2.7 Ajout d'une ACP pour optimiser les performances</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.7.1-Chargement-des-bibliothèques-nécessaires\" data-toc-modified-id=\"2.7.1-Chargement-des-bibliothèques-nécessaires-5.7.1\">2.7.1 Chargement des bibliothèques nécessaires</a></span></li><li><span><a href=\"#2.7.2-Préparer-les-données-à-leurs-manipulation-par-pySpark\" data-toc-modified-id=\"2.7.2-Préparer-les-données-à-leurs-manipulation-par-pySpark-5.7.2\">2.7.2 Préparer les données à leurs manipulation par pySpark</a></span></li><li><span><a href=\"#2.7.3-Détermination-du-nombre-minimal-de-composantes-requises-pour-expliquer-plus-de-95-%-de-la-variance\" data-toc-modified-id=\"2.7.3-Détermination-du-nombre-minimal-de-composantes-requises-pour-expliquer-plus-de-95-%-de-la-variance-5.7.3\">2.7.3 Détermination du nombre minimal de composantes requises pour expliquer plus de 95 % de la variance</a></span></li><li><span><a href=\"#2.7.4-Application-de-l'ACP-sur-le-nombre-de-composants-principaux-déterminés-précédemment\" data-toc-modified-id=\"2.7.4-Application-de-l'ACP-sur-le-nombre-de-composants-principaux-déterminés-précédemment-5.7.4\">2.7.4 Application de l'ACP sur le nombre de composants principaux déterminés précédemment</a></span></li><li><span><a href=\"#2.7.5-Sauvegarde-des-features-au-format-parquet\" data-toc-modified-id=\"2.7.5-Sauvegarde-des-features-au-format-parquet-5.7.5\">2.7.5 Sauvegarde des features au format parquet</a></span></li><li><span><a href=\"#2.7.6-Chargement-des-données-enregistrées-et-validation-du-résultat\" data-toc-modified-id=\"2.7.6-Chargement-des-données-enregistrées-et-validation-du-résultat-5.7.6\">2.7.6 Chargement des données enregistrées et validation du résultat</a></span></li></ul></li></ul></li><li><span><a href=\"#3.-Historique-Spark\" data-toc-modified-id=\"3.-Historique-Spark-6\">3. Historique Spark</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1-Suivi-de-l'avancement-des-tâches-avec-le-Serveur-d'Historique-Spark\" data-toc-modified-id=\"3.1-Suivi-de-l'avancement-des-tâches-avec-le-Serveur-d'Historique-Spark-6.1\">3.1 Suivi de l'avancement des tâches avec le Serveur d'Historique Spark</a></span></li><li><span><a href=\"#3.2-Test-de-mise-à-l'échelle-horizontale\" data-toc-modified-id=\"3.2-Test-de-mise-à-l'échelle-horizontale-6.2\">3.2 Test de mise à l'échelle horizontale</a></span></li></ul></li><li><span><a href=\"#4.-Résiliation-de-l'instance-EMR\" data-toc-modified-id=\"4.-Résiliation-de-l'instance-EMR-7\">4. Résiliation de l'instance EMR</a></span></li><li><span><a href=\"#5.-Cloner-le-serveur-EMR-(si-besoin)\" data-toc-modified-id=\"5.-Cloner-le-serveur-EMR-(si-besoin)-8\">5. Cloner le serveur EMR (si besoin)</a></span></li><li><span><a href=\"#6.-Arborescence-du-serveur-S3-à-la-fin-du-projet\" data-toc-modified-id=\"6.-Arborescence-du-serveur-S3-à-la-fin-du-projet-9\">6. Arborescence du serveur S3 à la fin du projet</a></span></li><li><span><a href=\"#7.-Conclusion\" data-toc-modified-id=\"7.-Conclusion-10\">7. Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fb6d44",
   "metadata": {},
   "source": [
    "# 1. Configuration cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407871c4",
   "metadata": {},
   "source": [
    " \n",
    "## 1.1 Choix du prestataire cloud : AWS\n",
    "\n",
    "Le prestataire le plus connu et qui offre à ce jour l'offre la plus large dans le cloud computing est **Amazon Web Services** (AWS). Certaines de leurs offres sont parfaitement adaptées à notre problématique, c'est pourquoi j'utiliserai leurs services.\n",
    "\n",
    "Le but premier est de pouvoir, grâce à AWS, <u>louer de la puissance de calcul à la demande</u>. L'idée étant de pouvoir, quel que soit la charge de travail, obtenir suffisamment de puissance de calcul pour pouvoir traiter nos images, même si le volume de données venait à fortement augmenter.\n",
    "\n",
    "De plus, la capacité d'utiliser cette puissance de calcul à la demande permet de diminuer drastiquement les coûts si on les compare à une location d'un nombre défini de serveurs pour une période fixe (1 mois, 1 année, par exemple)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14f04b3",
   "metadata": {},
   "source": [
    "## 1.2 Choix de la solution technique : EMR\n",
    "\n",
    "Plusieurs solutions s'offrent à nous :\n",
    "1. Solution **IAAS** (Infrastructure AS A Service)\n",
    " - Dans cette configuration **AWS** met à notre disposition des serveurs vierges, appelés **instance EC2**, sur lesquels nous avons un accès administrateur.  \n",
    "   Pour faire simple, il est possible avec cette solution de reproduire pratiquement à l'identique celle précédemment mise en œuvre en local sur notre machine.\n",
    "   \n",
    "   Ainsi, sur ce type de serveurs, il nous faut installer nous-même l'intégralité des outils, puis exécuter notre script :\n",
    "     - Installation de **Spark**, **Java** etc...\n",
    "     - Installation de **Python**.\n",
    "     - Installation de **Jupyter Notebook**.\n",
    "     - Installation des **librairies complémentaires**.\n",
    "     - Installation des **librairies nécessaires à toutes les machines (workers) du cluster**.<br /><br /> \n",
    "\n",
    "  - Avantages :\n",
    "      - Liberté totale de mise en œuvre de la solution.\n",
    "      - Facilité de déploiement à partir d'un modèle qui s'exécute en local sur une machine linux.<br /><br />   \n",
    "\n",
    "  - Inconvénients :\n",
    "      - Chronophage : Nécessité d'installer et de configurer toute la solution.\n",
    "      - Possible problèmes techniques à l'installation des outils (des problématiques qui n'existaient pas en local sur notre machine peuvent apparaitre sur le serveur EC2)\n",
    "      - Solution nécessitant une maintenance périodique : il faudra veiller à la mise à jour des outils et éventuellement devoir réinstaller Spark, Java etc... <br /><br /> \n",
    "\n",
    "2. Solution **PAAS** (Plateforme As A Service)\n",
    " - **AWS** fournit énormément de services différents, dans l'un de ceux-là il existe une offre qui permet de louer des **instances EC2** à la demande avec des applications préinstallées et configurées : il s'agit du **service EMR** :\n",
    "     - **Spark** y sera déjà installé.\n",
    "     - **Tensorflow** et **JupyterHub** sont proposés en option.\n",
    "     - Des **packages complémentaires** peuvent aussi être installé à l'initialisation du serveur, au moyen d'un petit fichier de bootstrap, **sur l'ensemble des machines du cluster**.<br /><br />\n",
    "       \n",
    " - Avantages :\n",
    "     - Facilité de mise en œuvre : Il suffit de très peu de configuration pour obtenir un environnement parfaitement fonctionnel.\n",
    "     - Rapidité de déploiement : Une fois la première configuration réalisée, il est très facile et très rapide de recréer des clusters à l'identique qui seront disponibles presque instantanément (le temps d'instancier les serveurs soit environ 15/20 minutes selon le compte et la disponibilité des serveurs).\n",
    "     - Solutions matérielles et logicielles optimisées par les ingénieurs d'AWS : Les versions installées devraient fonctionner \"out-of-the-box\" et l'architecture proposée est déjà optimisée.<br />\n",
    "           => Solution stable.\n",
    "     - Solution évolutive : Chaque nouvelle instanciation obtient des paquets à jour, en étant garanti de leur compatibilité avec le reste de l’environnement.\n",
    "     - Plus sécurisé : Les éventuels patchs de sécurité seront automatiquement mis à jour à chaque nouvelle instanciation du cluster EMR.<br /><br />\n",
    "       \n",
    " - Inconvénients :\n",
    "     - Peut-être un certain manque de liberté sur la version des paquets disponibles ?  \n",
    "       Bien que, dans le cadre de ce projet, ce problème ne s'est pas fait ressentir.<br /><br />\n",
    "\n",
    "=> La solution <u>**PAAS** est retenu par le biais du service **EMR** d'Amazon Web Services</u>.<br />\n",
    "Elle semble bien adaptée à notre problématique et permet une mise en œuvre à la fois plus rapide et plus efficace que la solution IAAS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0290d401",
   "metadata": {},
   "source": [
    "## 1.3 Choix de la solution de stockage des données : Amazon S3\n",
    "\n",
    "Amazon propose une solution très efficace pour la gestion du stockage des données : **Amazon S3 (Amazon Simple Storage Service)**.\n",
    "\n",
    "Il pourrait être tentant de stocker les données sur l'espace alloué par le serveur **EC2**, mais si aucune mesure n'est prise pour les sauvegarder ensuite sur un autre support, <u>les données seront perdues</u> lorsque le serveur est résilié. \n",
    "De fait, si l'on décide d'utiliser l'espace disque du serveur EC2, il faudra imaginer une solution pour sauvegarder les données avant la résiliation du serveur. De plus, nous serions exposés à certaines problématiques si nos données venaient à **saturer** l'espace disponible de nos serveurs (ralentissements, disfonctionnements).\n",
    "\n",
    "=> <u>Utiliser **Amazon S3** permet de s'affranchir de toutes ces problématiques</u> : L'espace disque disponible est **illimité** et **indépendant de nos serveurs EC2**. L'accès aux données est aussi **très rapide** car elles restent dans l'environnement d'AWS et il est possible de <u>choisir la même région géographique pour les serveurs **EC2** et **S3**</u>.\n",
    "\n",
    "De plus, comme nous le verrons dans ce notebook, il est possible d'accéder aux données sur **S3** de la même manière que l'on **accède aux données sur un disque local**, grâce à l'utilisation d'un **PATH au format s3://...**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b1469b",
   "metadata": {},
   "source": [
    "## 1.4 Configuration de l'environnement de travail\n",
    "\n",
    "La première étape est d'installer et de configurer [**AWS Cli**](https://aws.amazon.com/fr/cli/), il s'agit de l'**interface en ligne de commande d'AWS**. Elle nous permet d'**interagir avec les différents services d'AWS**, comme **S3** par exemple.\n",
    "\n",
    "Pour pouvoir utiliser **AWS Cli**, il faut le configurer en créant au préalable un utilisateur avec les autorisations dont nous avons besoin. Dans ce projet, il faut que l'utilisateur ait à minima un contrôle total sur le service S3.  \n",
    "*NB: La gestion des utilisateurs et de leurs droits s'effectue via le service **AMI** d'AWS.*\n",
    "\n",
    "Une fois l'utilisateur créé et ses autorisations configurées nous créons une **paire de clés EC2** qui nous permettra de nous **connecter sans à avoir à saisir systématiquement notre login/mot de passe**.  \n",
    "\n",
    "Il est également nécessaire de configurer l'**accès SSH** à nos futurs serveurs EC2. Là aussi, via un système de clés qui nous dispense de devoir nous authentifier \"à la main\" à chaque connexion.\n",
    "\n",
    "*NB: Des indications peuvent être trouvées ici : [Réalisez des calculs distribués sur des données massives / Découvrez Amazon Web Services](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308686-decouvrez-amazon-web-services#/id/r-4355822)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc2b62a",
   "metadata": {},
   "source": [
    "## 1.5 Upload de nos données sur S3\n",
    "\n",
    "Les outils maintenant configurés, il reste à  uploader nos données de travail dans un bucket Amazon S3. Ce sont les données contenues dans le dossier **Test** du [jeu de données du projet](https://www.kaggle.com/moltean/fruits/download) qui seront téléversées.  \n",
    "*NB: Indications sur la façon de faire ici :* [Réalisez des calculs distribués sur des données massives / Stockez des données sur S3](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308691-stockez-des-donnees-sur-s3)\n",
    "\n",
    "1. La première étape consiste à **créer un bucket sur S3** dans lequel seront téléversées les données:\n",
    "   - **aws s3 mb s3://oc-ds-p8-data**\n",
    "\n",
    "   *NB : Pour vérifier que le bucket à bien été créé, entrez la commande : **aws s3 ls*** <br />\n",
    "   *Si le nom du bucket s'affiche alors c'est qu'il a été correctement créé.*<br /><br />\n",
    "\n",
    "2. Le contenu du dossier \"**test**\" est téléversé dans un répertoire du même nom (\"**test**\") dans notre bucket \"**oc-ds-p8-data**\":\n",
    "\n",
    "   - jeu de données échantillon :\n",
    "   \n",
    "         a. cd /media/sf_P8/GitRepo/datasets/test_local_sample  \n",
    "         b. aws s3 cp --recursive . s3://oc-ds-p8-data/test (\"--recursive\" is required since there are many folders to copy)  \n",
    "            NB : Vérification la présence du dossier dans le bucket avec la commande : aws s3 ls s3://oc-ds-p8-data  \n",
    "         c. aws sync . s3://oc-ds-p8-data/test (Synchronisation des répertoires).  \n",
    "            NB : Penser à se mettre dans le bon répertoire en local au préalable.\n",
    "\n",
    "   - jeu de données complet :\n",
    "\n",
    "         a. cd /media/sf_P8/GitRepo/datasets  \n",
    "         b. aws s3 cp --recursive test s3://oc-ds-p8-data (\"--recursive\" is required since there are many folders to copy)\n",
    "            NB : Vérification la présence du dossier dans le bucket avec la commande : aws s3 ls s3://oc-ds-p8-data  \n",
    "         c. aws sync . s3://oc-ds-p8-data/test (Synchronisation des répertoires).  \n",
    "            NB : Penser à se mettre dans le bon répertoire en local au préalable.\n",
    "\n",
    "*NB : La commande **sync** permet de synchroniser notre répertoire local avec celui correspondant dans notre bucket S3.*\n",
    "\n",
    "=> <u>Nos données de travail sont maintenant disponibles dans notre stockage S3</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fda0fdc",
   "metadata": {},
   "source": [
    "## 1.6 Configuration du serveur EMR\n",
    "\n",
    "*NB : Pour configurer et lancer un serveur EMR, des indications sont disponible à cette adresse :  \n",
    "[Réalisez des calculs distribués sur des données massives / Déployez un cluster de calculs distribués](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308696-deployez-un-cluster-de-calculs-distribues), même si l'interface web d'Amazon a changé depuis.*\n",
    "\n",
    "<u>Je détaillerai ici les étapes particulières qui nous permettent de configurer le serveur selon nos besoins</u> :\n",
    "\n",
    "*NB : Certaines captures d'écran ont été ajoutées ou mises à jour depuis la réalisation de ce projet, c'est pourquoi les dates affichées peuvent varier dans certaines captures.*\n",
    "\n",
    "1. Cliquez sur Créer un cluster.\n",
    "\n",
    "<center><img src=\"./img/EMR_creer.png\" width=\"675\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b8abc7",
   "metadata": {},
   "source": [
    "### 1.6.1 Étape 1 : Logiciels et étapes\n",
    "\n",
    "#### 1.6.1.1 Configuration des logiciels\n",
    "\n",
    "Sélection des paquets dont nous aurons besoin (voir la capture d'écran) :\n",
    "\n",
    "1. Choix du nom du cluster.\n",
    "2. Sélection de la dernière version d'**EMR**, soit la version **6.13.0** au moment où je rédige ce document.\n",
    "3. Sélection d'**Hadoop** et de **Spark** qui seront préinstallés dans leur version la plus récente.\n",
    "4. Nous aurons également besoin de **TensorFlow** pour importer notre modèle et réaliser le **transfert learning**.\n",
    "5. Enfin, sélection de l'application **JupyterHub** pour ouvrir et executer notre **notebook Jupyter**. <br />\n",
    "\n",
    "<center><img src=\"./img/EMR_configuration_logiciels.png\" width=\"675\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d99c10-6b36-4a21-846b-0c8911d5ad26",
   "metadata": {},
   "source": [
    "#### 1.6.1.2 Modifier les paramètres du logiciel\n",
    "\n",
    "Il est préférable d'ouvrir les notebooks </u>non pas sur l'espace disque de l'instance EC2 (comme ce serait le cas dans la configuration par défaut de JupyterHub), mais <u>directement depuis notre espace **Amazon S3**</u>.<br />\n",
    "  \n",
    "Pour ce faire, deux solutions s'offrent à nous</u> :\n",
    "1. Créer un **fichier de configuration JSON** à **téléversé sur S3**, puis entrer dans la configuration le chemin d’accès vers ce fichier.  \n",
    "   *NB: Configuration à entrer dans le fichier JSON :  \n",
    "   [{\"classification\":\"jupyter-s3-conf\",\"properties\":{\"s3.persistence.bucket\":\"oc-ds-p8-data\",\"s3.persistence.enabled\":\"true\"}}]*\n",
    "3. Entrer directement la configuration au format JSON.\n",
    " \n",
    "J'ai préféré opter pour la seconde solution. Le clonage de l'instance EMR en est facilité puisque la configuration se retrouve directement copiée dans la nouvelle (voir la capture ci-dessous).\n",
    "\n",
    "<center><img src=\"./img/EMR_parametres_logiciel.png\" width=\"675\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55bec9b",
   "metadata": {},
   "source": [
    "### 1.6.2 Étape 2 : Matériel\n",
    "\n",
    "Sélection du type et du nombre d'instances EC2.\n",
    "\n",
    "1. Sélection des instances de type **M5** qui sont des **instances de type équilibré**.\n",
    "2. Sélection d'instances de type **xlarge** qui sont les **moins onéreuses disponibles**.<br />\n",
    "\n",
    "<center><img src=\"./img/EMR_materiel.png\" width=\"675\" /></center>\n",
    "\n",
    "*NB : Laissez les 15 Gio de volume racine EBS. Il est aussi possible de retirer le groupe d'intance de tâches si souhaité.*  \n",
    "*NB : [Plus d'informations sur les instances M5 Amazon EC2](https://aws.amazon.com/fr/ec2/instance-types/m5/)*\n",
    "\n",
    "3. **1 instance Maître** (le driver) et **2 instances Principales** (les workers) soit **un total de 3 instance EC2** sélectionnées.<br />\n",
    "   *NB : Le driver est sous-entendu.*\n",
    "\n",
    "<center><img src=\"./img/EMR_materiel_02.png\" width=\"675\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56eef56",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1.6.3 Étape 3 : Paramètres généraux de cluster\n",
    "\n",
    "#### 1.6.3.1 Résiliation du cluster\n",
    "\n",
    "Pour des raisons pratique la résiliation manuelle est sélectionnée et la \"protection de la résiliation\" est décochée pour des raisons pratiques.\n",
    "\n",
    "<center><img src=\"./img/EMR_résiliation.png\" width=\"675\" /></center>\n",
    "\n",
    "#### 1.6.3.2 Actions d'amorçage\n",
    "\n",
    "A cette étape sont configurés **les paquets** nécessaires à l'exécution de notre notebook qu'il reste à installer via un **fichier de boostrap**.  \n",
    "*NB :* <u>*Réaliser cette étape à ce moment de la configuration permet d'installer les paquets sur l'ensemble des instances du cluster en une fois*</u>.\n",
    "\n",
    "- Création et téléversement du fichier de bootstrap sur S3 à la racine de notre **bucket \"oc-ds-p8-data\"** (Voir la capture ci-dessous) :\n",
    "    1. cd /media/sf_P8/GitRepo # On se rend dans le dossier choisi pour stocker le fichier en local.\n",
    "    2. touch bootstrap-emr.sh # Création du fichier de boostrap vide.\n",
    "    3. Editer ce fichier pour y mettre les commandes d'installation des différents paquets manquants :<br /> \n",
    "\n",
    "        \"\"\"<br />\n",
    "        #! /bin/bash<br />\n",
    "        sudo python3 -m pip install -U setuptools pip<br />\n",
    "        sudo python3 -m pip install wheel pillow pandas pyarrow boto3 s3fs fsspec #tensorflow pyspark<br />\n",
    "        \"\"\"<br />\n",
    " \n",
    "    4. cat bootstrap-emr.sh # Vérification du contenu du fichier.\n",
    "    5. aws s3 cp bootstrap-emr.sh s3://oc-ds-p8-data/ # Copie du fichier dans le bucket S3.\n",
    "    6. aws s3 ls s3://oc-ds-p8-data/ # Vérification de la présence du fichier dans le bucket.\n",
    "\n",
    "<center><img src=\"./img/EMR_amorcage.png\" width=\"675\" /></center>\n",
    "\n",
    "*NB : La procédure pour créer le fichier **bootstrap** qui contient l'ensemble des instructions d'installation de tous les paquets peut aussi être trouvées [ici](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308696-deployez-un-cluster-de-calculs-distribues#/id/r-4356490) au besoin.*\n",
    "\n",
    "Voici le contenu du fichier **bootstrap-emr.sh** créé : <br />\n",
    "\n",
    "#! /bin/bash  \n",
    "sudo python3 -m pip install -U setuptools  \n",
    "sudo python3 -m pip install -U pip  \n",
    "sudo python3 -m pip install wheel  \n",
    "sudo python3 -m pip install pillow  \n",
    "sudo python3 -m pip install pandas  \n",
    "sudo python3 -m pip install pyarrow  \n",
    "sudo python3 -m pip install boto3  \n",
    "sudo python3 -m pip install s3fs  \n",
    "sudo python3 -m pip install fsspec  \n",
    "\n",
    "Il s'agit simplement de commandes \"**pip install**\" qui **installent les bibliothèques manquantes** de la même manière que dans notre notebook en local.  \n",
    "Une fois encore, <u>il est nécessaire de réaliser ces actions à cette étape</u> pour que <u>les paquets soient installés sur l'ensemble des machines du cluster</u> et non pas uniquement sur le driver.  \n",
    "*NB :* <u>*Ce serait le cas, si ces commandes étaient directement exécutées dans le notebook JupyterHub ou dans la console EMR (connectée au driver).*</u>\n",
    "\n",
    "**setuptools** et **pip** sont mis à jour pour éviter des problèmes de compatibilité avec l'installation du paquet **pyarrow**.  \n",
    "\n",
    "#### 1.6.3.3 Journalisation\n",
    "\n",
    "Les journaux sont sauvegardés dans un dossier \"log\" présent dans le bucket S3.\n",
    "\n",
    "<center><img src=\"./img/EMR_journalisation.png\" width=\"675\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3457434e",
   "metadata": {},
   "source": [
    "### 1.6.4 Étape 4 : Sécurité\n",
    "\n",
    "#### 1.6.4.1 Connection SSH\n",
    "\n",
    "La **paire de clés EC2** créée précédemment permettra de se connecter en **ssh** à nos **instances EC2** sans avoir à entrer nos login/mot de passe. Les autres paramètres peuvent être laissé à leurs valeurs par défaut.  \n",
    "\n",
    "**=> La configuration de notre cluster est terminée.**\n",
    "\n",
    "<center><img src=\"./img/EMR_securite.png\" width=\"675\" /></center>\n",
    "\n",
    "#### 1.6.4.2 Configuration des rôles IAM\n",
    "\n",
    "Les fonctions de services sont créées automatiquement par Amazon EMR. \n",
    "\n",
    "<center><img src=\"./img/EMR_IAM_01.png\" width=\"675\" /></center>\n",
    "<center><img src=\"./img/EMR_IAM_02.png\" width=\"675\" /></center>\n",
    "\n",
    "*NB: Si la mise en place du cluster est avortée à cause de problème de permissions pour accéder aux fichiers du bucket S3 :*  \n",
    "1. *allez dans l'interface web du service IAM,*\n",
    "2. *cliquez sur le profil d'instance qui a été automatiquement créé lors de la configuration du cluster\n",
    "   (ex: \"AmazonEMR-InstanceProfile-20240204T023021\"),*\n",
    "4. *cliquez sur \"ajouter des autorisations\" > \"attacher des startégies\",*\n",
    "5. *recherchez et sélectionnez l'autorisation \"AmazonS3FullAccess\",*\n",
    "6. *cliquez sur \"ajouter des autorisation\".*\n",
    "7. *Maintenant, clonez le cluster précédent et relancez sa création avec les rôles créés précédemment.\n",
    "   Le problème devrait être résolu.*\n",
    "\n",
    "<center><img src=\"./img/EMR_IAM_03.png\" width=\"675\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17d7cce",
   "metadata": {},
   "source": [
    "## 1.7 Instanciation du serveur\n",
    "\n",
    "Il ne reste plus qu'à attendre que le serveur soit prêt. Cette étape peut prendre entre **7 et 20 minutes** selon le compte utilisé et la disponibilité des serveurs.\n",
    "\n",
    "Il est possible de suivre l'avancé du statut du **cluster EMR** en attendant (voir la capture suivante)  \n",
    "\n",
    "<center><img src=\"./img/EMR_instanciation.png\" /></center>  \n",
    "\n",
    "Si le statut fini par afficher en vert \"**En attente**\", l'instanciation s'est bien déroulée et le serveur est prêt à être utilisé.  \n",
    "\n",
    "<center><img src=\"./img/EMR_cluster_prêt.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39be1ab4",
   "metadata": {},
   "source": [
    "## 1.8 Création du tunnel SSH à l'instance EC2 (Maître)\n",
    "\n",
    "### 1.8.1 Création des autorisations sur les connexions entrantes\n",
    "\n",
    "Nous souhaitons maintenant pouvoir accéder à nos applications :\n",
    " - **JupyterHub** pour l'exécution de notre notebook.\n",
    " - **Serveur d'historique Spark** pour le suivi de l'exécution des tâches de notre script lorsqu'il sera lancé.\n",
    " \n",
    "Cependant, <u>ces applications ne sont accessibles que depuis le réseau local du driver</u>. Il faut donc **créer un tunnel SSH vers lui** pour y accéder.\n",
    "\n",
    "Par défaut, ce driver se situe derrière un firewall qui bloque l'accès en SSH. <u>Le port 22 correspond au port sur lequel écoute le serveur SSH, pour l'ouvrir dans le pare-feu, il faut modifier le **groupe de sécurité EC2 du driver**</u>.\n",
    "\n",
    "*NB : Des indications sur les différentes étapes à suivre peuvent être trouvées [ici](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308696-deployez-un-cluster-de-calculs-distribues#/id/r-4356512):*  \n",
    "*\"Il faudra que l'on se connecte en SSH au driver de notre cluster. Par défaut, ce driver se situe derrière un firewall qui bloque l'accès en SSH. Pour ouvrir le port 22 qui correspond au port sur lequel écoute le serveur SSH, il faut modifier le groupe de sécurité EC2 du driver. Sur la page de la console consacrée à EC2, dans l'onglet \"Réseau et sécurité\", cliquez sur \"Groupes de sécurité\". Vous allez devoir modifier le groupe de sécurité d’ElasticMapReduce-Master. Dans l'onglet \"Entrant\", ajoutez une règle SSH dont la source est \"N'importe où\" (ou \"Mon IP\" si vous disposez d'une adresse IP fixe).\"*\n",
    "\n",
    "<center><img src=\"./img/EMR_config_ssh_01.png\" width=\"1200\" /></center>\n",
    "\n",
    "Une fois cette étape réalisée vous devriez avoir une configuration semblable à la catpure ci-dessous :\n",
    "\n",
    "<center><img src=\"./img/EMR_config_ssh_02.png\" width=\"1200\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214a6e88",
   "metadata": {},
   "source": [
    "### 1.8.2 Création du tunnel ssh vers le Driver\n",
    "\n",
    "Il est maintenant possible d'établir le **tunnel SSH** vers le **driver**. Les informations de connexion fournis par Amazon sont à récupérer depuis la page du service EMR > Cluster > onglet Récapitulatif en cliquant sur \"**Activer la connexion Web**\".\n",
    "\n",
    "<center><img src=\"./img/EMR_tunnel_ssh_01.png\" width=\"1200\" /></center>\n",
    "\n",
    "La commande fournis par Amazon permet ensuite d'**établir le tunnel SSH** :\n",
    "\n",
    "<center><img src=\"./img/EMR_tunnel_ssh_02.png\" width=\"675\" /></center>\n",
    "\n",
    "Dans notre cas, la commande ne fonctionne pas tel quel. Il a fallu **l'adapter à la configuration locale**. La **clé ssh** se situe directement dans le dossier racine du projet. Il est, par conséquent, nécessaire de se placer dans ce dossier avant de lancer la commande. En conséquence, le tilde \"**~**\", qui représente le répertoire personnel dans linux, est superflu et doit être supprimé.<br />\n",
    "\n",
    "*NB: Les détails pour créer un tunnel SSH vers une instance EC2 peuvent être trouvés [ici](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives).* <br />\n",
    "\n",
    "Le port **5555** de notre machine locale est sélectionné pour recevoir les commandes à transférer à notre cluster EC2 via le tunnel ssh (\"Redirection dynamique de port\").  \n",
    "*NB : Le choix du port n'est pas très important du moment qu'il reste dans la plage des ports accessibles par votre routeur.*<br />\n",
    "\n",
    "*NB : L'agument **-N** causait un problème de compatibilité (liste des arguments et leurs significations disponibles [ici](https://explainshell.com/explain?cmd=ssh+-L+-N+-f+-l+-D)), il a été retiré de la commande fournie par Amazon.*\n",
    "\n",
    "=> In fine, la commande suivante est entrée dans le terminal pour établir le tunnel ssh entre la machine locale et le cluster EC2 dans le cloud : \"**ssh -i oc_ds_p8-ssh_key_pair.pem -D 5555 hadoop@ec2-35-180-119-160.eu-west-3.compute.amazonaws.com**\"  \n",
    "*NB : L'URL change d'une instance à une autre*.  \n",
    "*NB : Penser à se placer dans le dossier contenant la clé ssh \"oc_ds_p8-ssh_key_pair.pem\" avant de lancer la commande :  \n",
    "      \"cd /media/sf_P8/GitRepo\".*<br />\n",
    "\n",
    "Maintenant, il suffit de taper \"**yes**\" pour valider la connexion. Si elle s'est bien établie, on obtient le résultat suivant :\n",
    "\n",
    "<center><img src=\"./img/EMR_connexion_ssh_01.png\" width=\"1200\" /></center>\n",
    "\n",
    "**=> Le tunnel ssh s'est correctement établi avec le driver via le port local \"5555\".**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8acee6",
   "metadata": {},
   "source": [
    "### 1.8.3 Configuration de FoxyProxy\n",
    "\n",
    "Une dernière étape est nécessaire pour accéder à nos applications, en demandant à notre navigateur d'emprunter le tunnel ssh grâce à l'extension **FoxyProxy**.  \n",
    "*NB : Cliquer sur le lien pour suivre les instructions de configuration de l'extension [lien](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308701-realisez-la-maintenance-dun-cluster#/id/r-4356554)*.\n",
    "\n",
    "Dans la rubrique **\"Proxies\"** de l'extension cliquez sur **Add**, puis renseigner les éléments comme dans la capture ci-dessous :\n",
    "\n",
    "<center><img src=\"./img/EMR_foxyproxy_config.png\" width=\"1200\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b7d66a",
   "metadata": {},
   "source": [
    "### 1.8.4 Accès aux applications du serveur EMR via le tunnel ssh\n",
    "\n",
    "Le navigateur peut maintenant se connecter aux applications de notre cluster via l'extension **FoxyProxy** par le tunnel SSH établi :\n",
    "\n",
    "<center><img src=\"./img/EMR_foxyproxy_activation.png\" width=\"400\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26cb15e",
   "metadata": {},
   "source": [
    "## 1.9 Connexion au notebook JupyterHub\n",
    "\n",
    "Pour se connecter à **JupyterHub** en vue d'exécuter notre **notebook**, il faut commencer par cliquer sur l'application **JupyterHub** apparue depuis que notre navigateur est connecté en ssh à notre cluster (actualisez la page si ce n’est pas le cas).\n",
    "\n",
    "<center><img src=\"./img/EMR_jupyterhub_connexion_01.png\" width=\"1200\" /></center>\n",
    "\n",
    "Un fois passé les éventuels avertissements de sécurité, nous arrivons sur une page de connexion.\n",
    "    \n",
    "On se connecte avec les informations par défaut :\n",
    " - login : **jovyan**\n",
    " - password : **jupyter**\n",
    " \n",
    "<center><img src=\"./img/EMR_jupyterhub_connexion_02.png\" width=\"675\" /></center>\n",
    "\n",
    "Nous arrivons ensuite dans un dossier vierge de notebook. Il suffit d'en créer un en cliquant sur \"**New**\" en haut à droite ou d'en téléverser un directement dans notre **bucket S3**.  \n",
    "\n",
    "![Liste et création des notebook](img/EMR_jupyterhub_creer_notebooks.png)\n",
    "\n",
    "Grâce à la **persistance** paramétrée à l'instanciation du cluster nous sommes actuellement dans l'arborescence du **bucket**.\n",
    "\n",
    "Le notebook déjà rédigé en local est importé sur S3 et ouvert depuis **l'interface JupyterHub** pour être adapté à ce nouvel environnement (changement des chemins des fichier vers S3, optimisation et suppression des étapes de vérification visuels superflues afin d'éviter d'exécuter le code spark inutilement...) et exécuter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12d1a9d",
   "metadata": {},
   "source": [
    "# 2. Exécution du code\n",
    "\n",
    "Cette partie du code est exécutée depuis l'application **JupyterHub hébergé sur notre cluster EMR**. Pour ne pas alourdir inutilement les explications du **notebook**, les étapes communes avec la version locale ne sont pas réexpliquées. <br />\n",
    "\n",
    "*NB : Avant de commencer, il faut s'assurer d'utiliser le **kernel pyspark** dans le notebook.  \n",
    "De plus, **en utilisant ce kernel, une session spark est créé à l'exécution de la première cellule**.  \n",
    "Il n'est donc **plus nécessaire d'exécuter le code \"spark = (SparkSession ...)\"** comme lors de l'exécution de notre notebook en local sur notre VM Kubuntu.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e759c1e",
   "metadata": {},
   "source": [
    "## 2.1 Démarrage de la session Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643ff851",
   "metadata": {},
   "source": [
    "Avant de démarrer l'application Spark, il est nécessaire de paramétrer la quantité de mémoire disponible pour le driver : </br> https://stackoverflow.com/questions/58062824/session-isnt-active-pyspark-in-an-aws-emr-cluster. </br>\n",
    "\n",
    "En effet, par défaut, le driver peut accéder à 1 GB de mémoire mais, à la fin du notebook, il n'en aura pas assez pour charger le dataframe créé et stocké au format parquet dans notre bucket S3.\n",
    "\n",
    "*NB : Cette étape de configuration n'est pas forcément nécessaire et peut être supprimée selon les cas d'usage.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bffeab9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '4000M', 'proxyUser': 'jovyan', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "No active sessions."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\"driverMemory\": \"4000M\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "508e4807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>12</td><td>application_1694534598976_0013</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-44-84.eu-west-3.compute.internal:20888/proxy/application_1694534598976_0013/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-34-144.eu-west-3.compute.internal:8042/node/containerlogs/container_1694534598976_0013_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "606a17a1a0c7410a97e0ea5ca828569f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c324fd3a1844b0bf095f4af7eedea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# L'exécution de cette cellule démarre l'application Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aba202f",
   "metadata": {},
   "source": [
    "<u>Affichage des informations sur la session en cours et liens vers Spark UI</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb788991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '4000M', 'proxyUser': 'jovyan', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>12</td><td>application_1694534598976_0013</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-44-84.eu-west-3.compute.internal:20888/proxy/application_1694534598976_0013/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-34-144.eu-west-3.compute.internal:8042/node/containerlogs/container_1694534598976_0013_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6fa8eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe59c93071fb40779aab0565c6b304a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory available per executor: 4743M"
     ]
    }
   ],
   "source": [
    "print(\"Memory available per executor:\", spark.conf.get(\"spark.executor.memory\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9ecc359",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60d542327344b4a931885dba5cacb12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   --- SparkSession config ---\n",
      "[('spark.blacklist.decommissioning.enabled', 'true'), ('spark.eventLog.enabled', 'true'), ('spark.sql.parquet.output.committer.class', 'com.amazon.emr.committer.EmrOptimizedSparkSqlParquetOutputCommitter'), ('spark.blacklist.decommissioning.timeout', '1h'), ('spark.app.startTime', '1694541140928'), ('spark.yarn.appMasterEnv.SPARK_PUBLIC_DNS', '$(hostname -f)'), ('spark.repl.class.uri', 'spark://ip-172-31-34-144.eu-west-3.compute.internal:38481/classes'), ('spark.sql.emr.internal.extensions', 'com.amazonaws.emr.spark.EmrSparkSessionExtensions'), ('spark.eventLog.dir', 'hdfs:///var/log/spark/apps'), ('spark.driver.host', 'ip-172-31-34-144.eu-west-3.compute.internal'), ('spark.yarn.app.container.log.dir', '/var/log/hadoop-yarn/containers/application_1694534598976_0013/container_1694534598976_0013_01_000001'), ('spark.executorEnv.PYTHONPATH', '{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9.7-src.zip'), ('spark.sql.warehouse.dir', 'hdfs://ip-172-31-44-84.eu-west-3.compute.internal:8020/user/spark/warehouse'), ('spark.history.fs.logDirectory', 'hdfs:///var/log/spark/apps'), ('spark.ui.filters', 'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'), ('spark.executor.memory', '4743M'), ('spark.yarn.dist.jars', 'file:///usr/lib/livy/rsc-jars/kryo-shaded-4.0.2.jar,file:///usr/lib/livy/rsc-jars/livy-api-0.7.1-incubating.jar,file:///usr/lib/livy/rsc-jars/livy-rsc-0.7.1-incubating.jar,file:///usr/lib/livy/rsc-jars/livy-thriftserver-session-0.7.1-incubating.jar,file:///usr/lib/livy/rsc-jars/minlog-1.3.0.jar,file:///usr/lib/livy/rsc-jars/netty-all-4.1.17.Final.jar,file:///usr/lib/livy/rsc-jars/objenesis-2.5.1.jar,file:///usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar,file:///usr/lib/livy/repl_2.12-jars/kryo-shaded-4.0.2.jar,file:///usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.7.1-incubating.jar,file:///usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.7.1-incubating.jar,file:///usr/lib/livy/repl_2.12-jars/minlog-1.3.0.jar,file:///usr/lib/livy/repl_2.12-jars/objenesis-2.5.1.jar'), ('spark.executor.extraLibraryPath', '/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native'), ('spark.emr.default.executor.memory', '4743M'), ('spark.repl.class.outputDir', '/mnt1/yarn/usercache/livy/appcache/application_1694534598976_0013/container_1694534598976_0013_01_000001/tmp/spark6170334078556514712'), ('spark.yarn.secondary.jars', 'kryo-shaded-4.0.2.jar,livy-api-0.7.1-incubating.jar,livy-rsc-0.7.1-incubating.jar,livy-thriftserver-session-0.7.1-incubating.jar,minlog-1.3.0.jar,netty-all-4.1.17.Final.jar,objenesis-2.5.1.jar,commons-codec-1.9.jar,livy-core_2.12-0.7.1-incubating.jar,livy-repl_2.12-0.7.1-incubating.jar'), ('spark.hadoop.yarn.timeline-service.enabled', 'false'), ('spark.emr.default.executor.cores', '2'), ('spark.executor.id', 'driver'), ('spark.executor.extraJavaOptions', \"-Djava.net.preferIPv6Addresses=false -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p' -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\"), ('spark.yarn.historyServer.address', 'ip-172-31-44-84.eu-west-3.compute.internal:18080'), ('spark.driver.port', '38481'), ('spark.hadoop.mapreduce.output.fs.optimized.committer.enabled', 'true'), ('spark.decommissioning.timeout.threshold', '20'), ('spark.sql.catalogImplementation', 'hive'), ('spark.yarn.dist.pyFiles', ''), ('spark.stage.attempt.ignoreOnDecommissionFetchFailure', 'true'), ('spark.app.name', 'livy-session-12'), ('spark.hadoop.fs.s3.getObject.initialSocketTimeoutMilliseconds', '2000'), ('spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version.emr_internal_use_only.EmrFileSystem', '2'), ('spark.app.id', 'application_1694534598976_0013'), ('spark.submit.deployMode', 'cluster'), ('spark.yarn.submit.waitAppCompletion', 'false'), ('spark.yarn.dist.files', 'file:/etc/hudi/conf.dist/hudi-defaults.conf'), ('spark.app.submitTime', '1694541124622'), ('spark.yarn.dist.archives', 'file:/usr/lib/spark/R/lib/sparkr.zip#sparkr'), ('spark.yarn.app.id', 'application_1694534598976_0013'), ('spark.yarn.maxAppAttempts', '1'), ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS', 'ip-172-31-44-84.eu-west-3.compute.internal'), ('spark.sql.hive.metastore.sharedPrefixes', 'com.amazonaws.services.dynamodbv2'), ('spark.app.attempt.id', '1'), ('spark.sql.parquet.fs.optimized.committer.optimization-enabled', 'true'), ('spark.executor.extraClassPath', '/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/redshift/jdbc/RedshiftJDBC.jar:/usr/share/aws/redshift/spark-redshift/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/docker/usr/lib/hadoop-lzo/lib/*:/docker/usr/lib/hadoop/hadoop-aws.jar:/docker/usr/share/aws/aws-java-sdk/*:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/docker/usr/share/aws/emr/security/conf:/docker/usr/share/aws/emr/security/lib/*:/docker/usr/share/aws/redshift/jdbc/RedshiftJDBC.jar:/docker/usr/share/aws/redshift/spark-redshift/lib/*:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/docker/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar'), ('spark.driver.extraLibraryPath', '/usr/lib/hadoop/lib/native:/usr/lib/hadoop-lzo/lib/native:/docker/usr/lib/hadoop/lib/native:/docker/usr/lib/hadoop-lzo/lib/native'), ('spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored.emr_internal_use_only.EmrFileSystem', 'true'), ('spark.livy.spark_major_version', '3'), ('spark.history.ui.port', '18080'), ('spark.shuffle.service.enabled', 'true'), ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES', 'http://ip-172-31-44-84.eu-west-3.compute.internal:20888/proxy/application_1694534598976_0013'), ('spark.yarn.tags', 'livy-session-12-xW0TjHvY'), ('spark.driver.defaultJavaOptions', \"-XX:OnOutOfMemoryError='kill -9 %p'\"), ('spark.resourceManager.cleanupExpiredHost', 'true'), ('spark.executor.defaultJavaOptions', \"-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:OnOutOfMemoryError='kill -9 %p'\"), ('spark.executor.cores', '2'), ('spark.files.fetchFailure.unRegisterOutputOnHost', 'true'), ('spark.driver.extraClassPath', '/usr/lib/hadoop-lzo/lib/*:/usr/lib/hadoop/hadoop-aws.jar:/usr/share/aws/aws-java-sdk/*:/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/usr/share/aws/emr/security/conf:/usr/share/aws/emr/security/lib/*:/usr/share/aws/redshift/jdbc/RedshiftJDBC.jar:/usr/share/aws/redshift/spark-redshift/lib/*:/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar:/docker/usr/lib/hadoop-lzo/lib/*:/docker/usr/lib/hadoop/hadoop-aws.jar:/docker/usr/share/aws/aws-java-sdk/*:/docker/usr/share/aws/emr/goodies/lib/emr-spark-goodies.jar:/docker/usr/share/aws/emr/security/conf:/docker/usr/share/aws/emr/security/lib/*:/docker/usr/share/aws/redshift/jdbc/RedshiftJDBC.jar:/docker/usr/share/aws/redshift/spark-redshift/lib/*:/docker/usr/share/aws/hmclient/lib/aws-glue-datacatalog-spark-client.jar:/docker/usr/share/java/Hive-JSON-Serde/hive-openx-serde.jar:/docker/usr/share/aws/sagemaker-spark-sdk/lib/sagemaker-spark-sdk.jar:/docker/usr/share/aws/emr/s3select/lib/emr-s3-select-spark-connector.jar'), ('spark.master', 'yarn'), ('spark.ui.port', '0'), ('spark.submit.pyFiles', ''), ('spark.dynamicAllocation.enabled', 'true'), ('spark.driver.extraJavaOptions', \"-Djava.net.preferIPv6Addresses=false -XX:OnOutOfMemoryError='kill -9 %p' -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false -XX:OnOutOfMemoryError='kill -9 %p'\"), ('spark.yarn.isPython', 'true'), ('spark.driver.memory', '4000M')]"
     ]
    }
   ],
   "source": [
    "# Check SparkSession config.\n",
    "print(\"   --- SparkSession config ---\")\n",
    "print(spark.sparkContext._conf.getAll())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ac9832",
   "metadata": {},
   "source": [
    "## 2.2 Installation des paquets\n",
    "\n",
    "Les paquets nécessaires ont été installés via l'étape de **bootstrap** lors de l'instanciation du serveur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce2588e",
   "metadata": {},
   "source": [
    "## 2.3 Configuration graphique du notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96c0cd56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f56a07102bf4ee5abe12424cc01b925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the width of the notebook sheet to the width of the window.\n",
    "#from IPython.display import display, HTML\n",
    "#display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ca5cd4",
   "metadata": {},
   "source": [
    "## 2.4 Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6e060e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fccb2e9e67f44a2f9c14d5120e245444",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import base libraries for data manipulations.\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "\n",
    "# Import libraries relative to the preprocessing steps.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# Import required pySpark libraries.\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split\n",
    "#from pyspark import StorageLevel\n",
    "#from pyspark.sql import SparkSession\n",
    "\n",
    "# Warnings management.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83663cbd",
   "metadata": {},
   "source": [
    "## 2.5 Définition des PATH pour charger les images et enregistrer les résultats\n",
    "\n",
    "Nous accédons directement à nos **données sur S3** comme si elles étaient **stockées localement**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46be859d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9968ea2702d43b7baf11f51e0a47096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUCKET_PATH:   s3://oc-ds-p8-data\n",
      "DATA_PATH:   s3://oc-ds-p8-data/test\n",
      "RESULT_PATH: s3://oc-ds-p8-data/result"
     ]
    }
   ],
   "source": [
    "# Paths.\n",
    "BUCKET_PATH = \"s3://oc-ds-p8-data\"\n",
    "DATA_PATH = BUCKET_PATH + \"/test\"\n",
    "RESULT_PATH = BUCKET_PATH + \"/result\"\n",
    "\n",
    "# Check.\n",
    "print(\"BUCKET_PATH: \" + BUCKET_PATH + \\\n",
    "      \"\\nDATA_PATH:   \" + DATA_PATH + \\\n",
    "      \"\\nRESULT_PATH: \" + RESULT_PATH\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf883c20",
   "metadata": {},
   "source": [
    "## 2.6 Traitement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffe93f5",
   "metadata": {},
   "source": [
    "### 2.6.1 Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e4b319a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45634edf1cb4baeb61bbfbfffd8a0b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import images.\n",
    "images = spark.read.format(\"binaryFile\")\\\n",
    "                   .option(\"pathGlobFilter\", \"*.jpg\")\\\n",
    "                   .option(\"recursiveFileLookup\", \"true\")\\\n",
    "                   .load(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b32ac34",
   "metadata": {},
   "source": [
    "Seul le **path** de l'image est conservé et une colonne contenant les **labels** de chaque image est ajoutée :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff5d1b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221ea20cd90041fcb4941b4351f78a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6 processor cores available within the whole cluster.\n",
      "The spark dataframe is splitted in 18 partitions."
     ]
    }
   ],
   "source": [
    "# Optimize the number of partitions in which split the dataframe.\n",
    "# NB: sc.defaultParallelism returns the number of processor cores available within the whole cluster.\n",
    "images = images.coalesce(sc.defaultParallelism * 3) \n",
    "\n",
    "# Check if the number of partition fits the number of processor cores.\n",
    "print(\"There are %i processor cores available within the whole cluster.\" % sc.defaultParallelism)\n",
    "print(\"The spark dataframe is splitted in %i partitions.\" % images.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a52ab808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5be530b7b00347a7b63791d8492e6e62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   --- Image new schema ---\n",
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n",
      "None"
     ]
    }
   ],
   "source": [
    "# Create the image labels column.\n",
    "images = images.withColumn('label', element_at(split(images['path'], '/'), -2))\n",
    "\n",
    "print(\"   --- Image new schema ---\")\n",
    "print(images.printSchema())\n",
    "#print()\n",
    "#print(\"   --- Check values of the new column: label ---\")\n",
    "#print(images.select('path', 'label').show(5, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f15b199",
   "metadata": {},
   "source": [
    "### 2.6.2 Préparation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec7c7165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8440d09b74064c9d810394cf5fc8d8b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model MobileNetV2 with pretrained weights over imagenet.\n",
    "model = MobileNetV2(weights='imagenet',\n",
    "                    include_top=True,\n",
    "                    input_shape=(224, 224, 3)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b9bc650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "027be63a72b149c096f656fe4639ef39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create the new model.\n",
    "new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bc0bf14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce4b288cdf8142d483618d74441c5de0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " Conv1 (Conv2D)                 (None, 112, 112, 32  864         ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " bn_Conv1 (BatchNormalization)  (None, 112, 112, 32  128         ['Conv1[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " Conv1_relu (ReLU)              (None, 112, 112, 32  0           ['bn_Conv1[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise (Depth  (None, 112, 112, 32  288        ['Conv1_relu[0][0]']             \n",
      " wiseConv2D)                    )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_BN (Ba  (None, 112, 112, 32  128        ['expanded_conv_depthwise[0][0]']\n",
      " tchNormalization)              )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_relu (  (None, 112, 112, 32  0          ['expanded_conv_depthwise_BN[0][0\n",
      " ReLU)                          )                                ]']                              \n",
      "                                                                                                  \n",
      " expanded_conv_project (Conv2D)  (None, 112, 112, 16  512        ['expanded_conv_depthwise_relu[0]\n",
      "                                )                                [0]']                            \n",
      "                                                                                                  \n",
      " expanded_conv_project_BN (Batc  (None, 112, 112, 16  64         ['expanded_conv_project[0][0]']  \n",
      " hNormalization)                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_expand (Conv2D)        (None, 112, 112, 96  1536        ['expanded_conv_project_BN[0][0]'\n",
      "                                )                                ]                                \n",
      "                                                                                                  \n",
      " block_1_expand_BN (BatchNormal  (None, 112, 112, 96  384        ['block_1_expand[0][0]']         \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block_1_expand_relu (ReLU)     (None, 112, 112, 96  0           ['block_1_expand_BN[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_pad (ZeroPadding2D)    (None, 113, 113, 96  0           ['block_1_expand_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_depthwise (DepthwiseCo  (None, 56, 56, 96)  864         ['block_1_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_1_depthwise_BN (BatchNor  (None, 56, 56, 96)  384         ['block_1_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_1_depthwise_relu (ReLU)  (None, 56, 56, 96)   0           ['block_1_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_1_project (Conv2D)       (None, 56, 56, 24)   2304        ['block_1_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_1_project_BN (BatchNorma  (None, 56, 56, 24)  96          ['block_1_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_expand (Conv2D)        (None, 56, 56, 144)  3456        ['block_1_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_2_expand_BN (BatchNormal  (None, 56, 56, 144)  576        ['block_2_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_2_expand_relu (ReLU)     (None, 56, 56, 144)  0           ['block_2_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_2_depthwise (DepthwiseCo  (None, 56, 56, 144)  1296       ['block_2_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_2_depthwise_BN (BatchNor  (None, 56, 56, 144)  576        ['block_2_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_2_depthwise_relu (ReLU)  (None, 56, 56, 144)  0           ['block_2_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_2_project (Conv2D)       (None, 56, 56, 24)   3456        ['block_2_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_2_project_BN (BatchNorma  (None, 56, 56, 24)  96          ['block_2_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_add (Add)              (None, 56, 56, 24)   0           ['block_1_project_BN[0][0]',     \n",
      "                                                                  'block_2_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_3_expand (Conv2D)        (None, 56, 56, 144)  3456        ['block_2_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_3_expand_BN (BatchNormal  (None, 56, 56, 144)  576        ['block_3_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_3_expand_relu (ReLU)     (None, 56, 56, 144)  0           ['block_3_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_3_pad (ZeroPadding2D)    (None, 57, 57, 144)  0           ['block_3_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_3_depthwise (DepthwiseCo  (None, 28, 28, 144)  1296       ['block_3_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_3_depthwise_BN (BatchNor  (None, 28, 28, 144)  576        ['block_3_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_3_depthwise_relu (ReLU)  (None, 28, 28, 144)  0           ['block_3_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_3_project (Conv2D)       (None, 28, 28, 32)   4608        ['block_3_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_3_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_3_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_3_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_4_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_4_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_4_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_4_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_4_depthwise (DepthwiseCo  (None, 28, 28, 192)  1728       ['block_4_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_4_depthwise_BN (BatchNor  (None, 28, 28, 192)  768        ['block_4_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_4_depthwise_relu (ReLU)  (None, 28, 28, 192)  0           ['block_4_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_4_project (Conv2D)       (None, 28, 28, 32)   6144        ['block_4_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_4_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_4_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_add (Add)              (None, 28, 28, 32)   0           ['block_3_project_BN[0][0]',     \n",
      "                                                                  'block_4_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_5_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_4_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_5_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_5_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_5_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_5_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_5_depthwise (DepthwiseCo  (None, 28, 28, 192)  1728       ['block_5_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_5_depthwise_BN (BatchNor  (None, 28, 28, 192)  768        ['block_5_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_5_depthwise_relu (ReLU)  (None, 28, 28, 192)  0           ['block_5_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_5_project (Conv2D)       (None, 28, 28, 32)   6144        ['block_5_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_5_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_5_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_5_add (Add)              (None, 28, 28, 32)   0           ['block_4_add[0][0]',            \n",
      "                                                                  'block_5_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_6_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_5_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_6_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_6_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_6_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_6_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_6_pad (ZeroPadding2D)    (None, 29, 29, 192)  0           ['block_6_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_6_depthwise (DepthwiseCo  (None, 14, 14, 192)  1728       ['block_6_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_6_depthwise_BN (BatchNor  (None, 14, 14, 192)  768        ['block_6_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_6_depthwise_relu (ReLU)  (None, 14, 14, 192)  0           ['block_6_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_6_project (Conv2D)       (None, 14, 14, 64)   12288       ['block_6_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_6_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_6_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_6_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_7_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_7_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_7_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_7_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_7_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_7_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_7_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_7_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_7_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_7_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_7_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_7_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_7_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_7_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_add (Add)              (None, 14, 14, 64)   0           ['block_6_project_BN[0][0]',     \n",
      "                                                                  'block_7_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_8_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_7_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_8_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_8_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_8_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_8_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_8_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_8_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_8_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_8_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_8_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_8_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_8_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_8_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_8_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_8_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_8_add (Add)              (None, 14, 14, 64)   0           ['block_7_add[0][0]',            \n",
      "                                                                  'block_8_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_9_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_8_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_9_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_9_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_9_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_9_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_9_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_9_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_9_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_9_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_9_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_9_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_9_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_9_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_9_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_9_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_9_add (Add)              (None, 14, 14, 64)   0           ['block_8_add[0][0]',            \n",
      "                                                                  'block_9_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_10_expand (Conv2D)       (None, 14, 14, 384)  24576       ['block_9_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_10_expand_BN (BatchNorma  (None, 14, 14, 384)  1536       ['block_10_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_10_expand_relu (ReLU)    (None, 14, 14, 384)  0           ['block_10_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_10_depthwise (DepthwiseC  (None, 14, 14, 384)  3456       ['block_10_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_10_depthwise_BN (BatchNo  (None, 14, 14, 384)  1536       ['block_10_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_10_depthwise_relu (ReLU)  (None, 14, 14, 384)  0          ['block_10_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_10_project (Conv2D)      (None, 14, 14, 96)   36864       ['block_10_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_10_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_10_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_10_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_11_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_11_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_11_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_11_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_11_depthwise (DepthwiseC  (None, 14, 14, 576)  5184       ['block_11_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_11_depthwise_BN (BatchNo  (None, 14, 14, 576)  2304       ['block_11_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_11_depthwise_relu (ReLU)  (None, 14, 14, 576)  0          ['block_11_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_11_project (Conv2D)      (None, 14, 14, 96)   55296       ['block_11_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_11_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_11_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_add (Add)             (None, 14, 14, 96)   0           ['block_10_project_BN[0][0]',    \n",
      "                                                                  'block_11_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_12_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_11_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_12_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_12_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_12_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_12_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_12_depthwise (DepthwiseC  (None, 14, 14, 576)  5184       ['block_12_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_12_depthwise_BN (BatchNo  (None, 14, 14, 576)  2304       ['block_12_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_12_depthwise_relu (ReLU)  (None, 14, 14, 576)  0          ['block_12_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_12_project (Conv2D)      (None, 14, 14, 96)   55296       ['block_12_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_12_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_12_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_12_add (Add)             (None, 14, 14, 96)   0           ['block_11_add[0][0]',           \n",
      "                                                                  'block_12_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_13_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_12_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_13_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_13_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_13_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_13_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_13_pad (ZeroPadding2D)   (None, 15, 15, 576)  0           ['block_13_expand_relu[0][0]']   \n",
      "                                                                                                  \n",
      " block_13_depthwise (DepthwiseC  (None, 7, 7, 576)   5184        ['block_13_pad[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_13_depthwise_BN (BatchNo  (None, 7, 7, 576)   2304        ['block_13_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_13_depthwise_relu (ReLU)  (None, 7, 7, 576)   0           ['block_13_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_13_project (Conv2D)      (None, 7, 7, 160)    92160       ['block_13_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_13_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_13_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_13_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_14_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_14_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_14_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_14_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_14_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_14_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_14_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_14_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_14_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_14_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_14_project (Conv2D)      (None, 7, 7, 160)    153600      ['block_14_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_14_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_14_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_add (Add)             (None, 7, 7, 160)    0           ['block_13_project_BN[0][0]',    \n",
      "                                                                  'block_14_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_15_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_14_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_15_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_15_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_15_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_15_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_15_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_15_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_15_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_15_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_15_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_15_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_15_project (Conv2D)      (None, 7, 7, 160)    153600      ['block_15_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_15_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_15_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_15_add (Add)             (None, 7, 7, 160)    0           ['block_14_add[0][0]',           \n",
      "                                                                  'block_15_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_16_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_15_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_16_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_16_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_16_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_16_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_16_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_16_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_16_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_16_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_16_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_16_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_16_project (Conv2D)      (None, 7, 7, 320)    307200      ['block_16_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_16_project_BN (BatchNorm  (None, 7, 7, 320)   1280        ['block_16_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " Conv_1 (Conv2D)                (None, 7, 7, 1280)   409600      ['block_16_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " Conv_1_bn (BatchNormalization)  (None, 7, 7, 1280)  5120        ['Conv_1[0][0]']                 \n",
      "                                                                                                  \n",
      " out_relu (ReLU)                (None, 7, 7, 1280)   0           ['Conv_1_bn[0][0]']              \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 1280)        0           ['out_relu[0][0]']               \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,257,984\n",
      "Trainable params: 2,223,872\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "# Show the structure of the model.\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ade09738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e81ce0482b34a49912350bd35dc0b7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Broadcast the weights of the model among all workers.\n",
    "broadcast_weights = sc.broadcast(new_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be8fe2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d92092efb0124dab8e07cf5f01e1f632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def model_fn ():\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a MobileNetV2 model with top layer removed \n",
    "    and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the MobileNetV2 model.\n",
    "    model = MobileNetV2(weights='imagenet',\n",
    "                        include_top=True,\n",
    "                        input_shape=(224, 224, 3))\n",
    "    \n",
    "    # Remove trainability from all layers.\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # Create the new model without the last classification layers.\n",
    "    new_model = Model(inputs=model.input,\n",
    "                      outputs=model.layers[-2].output)\n",
    "    \n",
    "    # Broadcast the defined weights to the model.\n",
    "    new_model.set_weights(broadcast_weights.value)\n",
    "    \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c032f135",
   "metadata": {},
   "source": [
    "### 2.6.3 Définition du processus de chargement des images et application de leur featurisation à travers l'utilisation de pandas UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "933100cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6f0d2e76c4450eadeeae1d8ce1172c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess (content):\n",
    "    \n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes to make them fit the input layer format requirements of the model.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Image opened in RAM and resized to the required format by the model.\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    \n",
    "    # Convert the image to a numpy array which can be given to the entry points of the model.\n",
    "    arr = img_to_array(img)\n",
    "    \n",
    "    # Normalize (min max scaler) values within the numpy array.\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "\n",
    "def featurize_series (model, series_of_contents_batch):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----------\n",
    "    \n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    A pd.Series of image features.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Preprocess all images from their byte format to a numpy array format with a resize to 224 x 224.\n",
    "    input = np.stack(series_of_contents_batch.map(preprocess))\n",
    "    \n",
    "    # Features' extraction over each image.\n",
    "    preds = model.predict(input)\n",
    "    \n",
    "    # For some layers, output features will be multi-dimensional tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    \n",
    "    return pd.Series(output)\n",
    "\n",
    "\n",
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf (series_of_contents_all_batches):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----------\n",
    "    \n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    \n",
    "    NB: The decorator allows the user to create a method that can be applied a dataframe with Spark and\n",
    "    specifies the type of the object returned: here a Spark DataFrame column of type ArrayType(FloatType).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    series_of_contents_all_batches: \n",
    "        This argument is an iterator over batches of data, where each batch is a pandas Series of image data.\n",
    "                              \n",
    "    \"\"\"\n",
    "    \n",
    "    # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "    # for multiple data batches. This amortizes the overhead of loading big models.\n",
    "    model = model_fn()\n",
    "    \n",
    "    # Loop over each batch.\n",
    "    for series_of_contents_batch in series_of_contents_all_batches:\n",
    "        \n",
    "        # The function returns all extracted features from each batch in a generator (a tuple).\n",
    "        yield featurize_series(model, series_of_contents_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23206e8",
   "metadata": {},
   "source": [
    "### 2.6.4 Exécutions des actions d'extractions de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5e07fd68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a10175102b204d1d8d8ed0f7b679b592",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the images' names, paths and features within a spark dataframe.\n",
    "sdf = images.select(col(\"path\"),\n",
    "                    col(\"label\"),\n",
    "                    featurize_udf(\"content\").alias(\"features\")\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e5f294e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755dd46aca954eeb9ff58816c3c35bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[path: string, label: string, features: array<float>]"
     ]
    }
   ],
   "source": [
    "# In order to save time and not re-do the spark dataframe at each action step,\n",
    "# the sdf is saved in cache and directly accessible.\n",
    "sdf.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919c5134",
   "metadata": {},
   "source": [
    "## 2.7 Ajout d'une ACP pour optimiser les performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d37fe50",
   "metadata": {},
   "source": [
    "### 2.7.1 Chargement des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7345f92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7139e27e7b3e47c0b51a04afb815fe8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Allow to convert features column's items to VectorUDT type managable by the PCA function of pySpark.\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Import the scaler.\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Import the PCA function of pySpark.\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "# Enable the Apache Arrow to convert pandas dataframes to pySpark dataframes.\n",
    "# NB: Otherwise expect the following error: \"TypeError: Can not infer for type...\".\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4351fc53",
   "metadata": {},
   "source": [
    "### 2.7.2 Préparer les données à leurs manipulation par pySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "551ad580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f2f9a6d0d140d8b15602b4b5281560",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   --- sdf schema ---\n",
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "None"
     ]
    }
   ],
   "source": [
    "### Convert the features column's items from the Array type to VectorUDT type managable by the PCA function of pySpark.\n",
    "\n",
    "# Create the UDF converter.\n",
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "\n",
    "# Convert.\n",
    "sdf = sdf.withColumn(\"features\", list_to_vector_udf(sdf[\"features\"]))\n",
    "\n",
    "# Check the spark dataframe.\n",
    "print(\"   --- sdf schema ---\")\n",
    "print(sdf.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7842a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c110dcde229f474dbbb40d54f856038a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   --- sdf schema ---\n",
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- norm_features: vector (nullable = true)\n",
      "\n",
      "None"
     ]
    }
   ],
   "source": [
    "### Standardization of data before its use by the PCA.\n",
    "\n",
    "# Initialize the scaler.\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"norm_features\")\n",
    "\n",
    "# Scale the images' vectors of features.\n",
    "sdf = scaler.fit(sdf).transform(sdf)\n",
    "\n",
    "# Check the resulting table structure.\n",
    "print(\"   --- sdf schema ---\")\n",
    "print(sdf.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d858c43",
   "metadata": {},
   "source": [
    "### 2.7.3 Détermination du nombre minimal de composantes requises pour expliquer plus de 95 % de la variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c08ef1ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "415708b0dcb7489abed3e62de4c6ddfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCAModel: uid=PCA_da8406515bcc, k=1280"
     ]
    }
   ],
   "source": [
    "# Set the number of PCA components over project each feature.\n",
    "# NB: In order to ensure to be able to get at least 95 % of the cumulated variance,\n",
    "#     the number of PCA component is set equal to the vectors' size describing all image features.\n",
    "tot_n_feat = 1280\n",
    "\n",
    "# Initialize the PCA.\n",
    "pca = PCA(k=tot_n_feat, inputCol=\"norm_features\", outputCol=\"pca_components\")\n",
    "\n",
    "# Generate the PCA model.\n",
    "pca_model = pca.fit(sdf.select(\"norm_features\"))\n",
    "\n",
    "# Check the model initialization.\n",
    "pca_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe5fa3a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b796b3a22a224280850f4432c3576454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the ratio of explained variance over each PCA components.\n",
    "expl_vars_ratios = pca_model.explainedVariance * 100\n",
    "\n",
    "# Show results.\n",
    "#show_sp = 25\n",
    "#print(\"Cumulated explained variance of the first principal components:\\n\", expl_vars_ratios[:show_sp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d21c9285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27fd5178e7a64f40a8059b9577b6c6bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   --- Result ---\n",
      "Minimal number of components to reach 95.00 % of cumulated explained variance: 557\n",
      "\n",
      "   --- Check ---\n",
      "Cumulated explained variance over 557 components: 95.01 %\n",
      "Cumulated explained variance over 556 components: 94.99 %"
     ]
    }
   ],
   "source": [
    "# Get the cumulated sum of the explained variance ratios.\n",
    "cum_sum_vars_ratios = expl_vars_ratios.cumsum()\n",
    "\n",
    "# Set the minimal cumulated explained variance threshold.\n",
    "cum_var_ratio_limit = 95\n",
    "\n",
    "# Get the minimal number of components to reach 95 % of the explained variance.\n",
    "valid_idx = np.where(cum_sum_vars_ratios >= cum_var_ratio_limit)[0]\n",
    "MIN_N_COMP = valid_idx[cum_sum_vars_ratios[valid_idx].argmin()] + 1\n",
    "\n",
    "# Check the results.\n",
    "print(\"   --- Result ---\")\n",
    "print(\"Minimal number of components to reach %.2f %% of cumulated explained variance: %i\" % (cum_var_ratio_limit, MIN_N_COMP))\n",
    "print()\n",
    "print(\"   --- Check ---\")\n",
    "print(\"Cumulated explained variance over %i components: %.2f \" % (MIN_N_COMP, expl_vars_ratios[:MIN_N_COMP].sum()) + \"%\")\n",
    "print(\"Cumulated explained variance over %i components: %.2f \" % (MIN_N_COMP-1, expl_vars_ratios[:MIN_N_COMP-1].sum()) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09669d6",
   "metadata": {},
   "source": [
    "### 2.7.4 Application de l'ACP sur le nombre de composants principaux déterminés précédemment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "438a46c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccd88193f9654461a9d07d13fb5f04e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.functions import vector_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31ca9ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2644a9c460e4641a02c6d84af5c2d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def trunc_array_udf (serie_batches):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----------\n",
    "    \n",
    "    This method is a Scalar Iterator pandas UDF wrapping the function which will truncate the vectors gotten after the PCA projection.\n",
    "    \n",
    "    NB: The decorator allows the user to create a method that can be applied over a dataframe with Spark and\n",
    "    specifies the type of the object returned: here a Spark DataFrame column of type ArrayType(FloatType).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    serie: \n",
    "        It is a pandas.Series object corresponding to one column of the spark dataframe.\n",
    "                              \n",
    "    \"\"\"       \n",
    "    \n",
    "    # Loop over each batch of numpy arrays.\n",
    "    for serie_batch in serie_batches:\n",
    "        \n",
    "        # It will return each processed batch before starting the next one.\n",
    "        yield pd.Series([v[:MIN_N_COMP] for v in serie_batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c034fb74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d3a1b06f4f4a12bd59b30c27723aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the features projected over the set number of PCA first components.\n",
    "sdf = pca_model.transform(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50e7ce9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c86af9fe16b24c4996a8918718809a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   --- sdf schema ---\n",
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- norm_features: vector (nullable = true)\n",
      " |-- pca_components: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n",
      "None"
     ]
    }
   ],
   "source": [
    "# Reconvert the PCA dense vectors objects back to their numpy array counter part then optimize their length.\n",
    "# NB: It was impossible to find a way to directly truncate the PCA projection vectors gotten as dense vector.\n",
    "sdf = sdf.withColumn(\"pca_components\", vector_to_array(\"pca_components\"))\\\n",
    "         .withColumn(\"pca_components\", trunc_array_udf(\"pca_components\"))\n",
    "\n",
    "# Check the resulting table structure.\n",
    "print(\"   --- sdf schema ---\")\n",
    "print(sdf.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "22689300",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ea6db466a2482e9d4b4d839c829d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+--------------------+--------------------+\n",
      "|                path|     label|            features|       norm_features|      pca_components|\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+\n",
      "|s3://oc-ds-p8-dat...|Watermelon|[0.0,0.2339710295...|[0.0,0.3369792072...|[-16.705645, 1.49...|\n",
      "|s3://oc-ds-p8-dat...|Watermelon|[0.08841755986213...|[0.16384188830824...|[-21.334585, 0.70...|\n",
      "|s3://oc-ds-p8-dat...|Watermelon|[0.0,0.4033172428...|[0.0,0.5808818512...|[-17.801802, 1.99...|\n",
      "|s3://oc-ds-p8-dat...|Watermelon|[0.00337656168267...|[0.00625692727712...|[-17.06633, 1.931...|\n",
      "|s3://oc-ds-p8-dat...|Watermelon|[0.00501113990321...|[0.00928587743882...|[-22.293434, 0.40...|\n",
      "+--------------------+----------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "# Get an overview of the final spark dataframe.\n",
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8d7e02",
   "metadata": {},
   "source": [
    "### 2.7.5 Sauvegarde des features au format parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6318d274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ebf7289da84ad28deb40cff68f663b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Save the dataframe in parquet files.\n",
    "sdf.write.mode(\"overwrite\").parquet(RESULT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5e4025",
   "metadata": {},
   "source": [
    "On peut également constater la présence des fichiers au format \"**parquet**\" sur le **serveur S3** :\n",
    "\n",
    "1. Interface web :\n",
    "\n",
    "<center><img src=\"./img/S3_Results.png\" width=\"1200\" /></center>\n",
    "\n",
    "2. AWS Cli :\n",
    "\n",
    "<center><img src=\"./img/S3_Results_AWSCli.png\" width=\"1200\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6233e283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e5da7954d5e42a391e23205c1463d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[path: string, label: string, features: vector, norm_features: vector, pca_components: array<float>]"
     ]
    }
   ],
   "source": [
    "# Once all computational were performed and the spark dataframe is saved, it can be removed from the cache.\n",
    "sdf.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9506f21",
   "metadata": {},
   "source": [
    "### 2.7.6 Chargement des données enregistrées et validation du résultat\n",
    "\n",
    "<u>On charge les données fraichement enregistrées dans un **DataFrame Pandas**</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "775f0cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a17fa7cfb5df4996ac6524709f7a8d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the spark dataframe stored in the saves parquet files and convert it to a pandas dataframe..\n",
    "df = spark.read.parquet(RESULT_PATH, engine='pyarrow').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f15070",
   "metadata": {},
   "source": [
    "<u>On affiche les 5 premières lignes du DataFrame</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8a1bcdeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "418058ffad134f438f8faedc0085e563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                path  ...                                     pca_components\n",
      "0     s3://oc-ds-p8-data/test/Nut Forest/413_100.jpg  ...  [-2.1294009685516357, -9.932955741882324, -4.7...\n",
      "1  s3://oc-ds-p8-data/test/Potato Red Washed/r_93...  ...  [-2.335909366607666, -8.036542892456055, 0.184...\n",
      "2           s3://oc-ds-p8-data/test/Quince/4_100.jpg  ...  [-2.4910759925842285, 5.476332664489746, 6.006...\n",
      "3  s3://oc-ds-p8-data/test/Tomato Yellow/r2_5_100...  ...  [5.4497294425964355, -7.835856914520264, 4.967...\n",
      "4  s3://oc-ds-p8-data/test/Apple Golden 2/55_100.jpg  ...  [1.6873353719711304, 2.8007776737213135, 2.128...\n",
      "\n",
      "[5 rows x 5 columns]\n",
      "Dataframe dimension: (22688, 5)"
     ]
    }
   ],
   "source": [
    "# Check the df.\n",
    "print(df.head())\n",
    "print(\"Dataframe dimension:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2794fca",
   "metadata": {},
   "source": [
    "On valide que la dimension du vecteur de caractéristiques des images est bien de dimension 1280 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0bb933b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e424aa8c8434c59989492297b5595aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the features' vectors: 1280\n",
      "Dimension of the PCA projection of the features' vectors: 557"
     ]
    }
   ],
   "source": [
    "# Get the dimension of the features before and after PCA.\n",
    "print(\"Dimension of the features' vectors:\", df.loc[0,'features'].shape[0])\n",
    "print(\"Dimension of the PCA projection of the features' vectors:\", len(df.loc[0,'pca_components']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4a7a0f",
   "metadata": {},
   "source": [
    "# 3. Historique Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72974aab",
   "metadata": {},
   "source": [
    "## 3.1 Suivi de l'avancement des tâches avec le Serveur d'Historique Spark\n",
    "\n",
    "Il est possible de voir l'avancement des tâches en cours avec le **serveur d'historique Spark**.\n",
    "\n",
    "<center><img src=\"./img/EMR_serveur_historique_spark_acces.png\" width=\"1200\" /></center>\n",
    "\n",
    "**Il est également possible de revenir et d'étudier les tâches qui ont été réalisées, afin de debugger et optimiser les futures tâches à réaliser.**\n",
    "\n",
    "Lorsque la commande \"**features_df.write.mode(\"overwrite\").parquet(PATH_Result) (Dans ce notebook => \"sdf.write.mode(\"overwrite\").parquet(RESULT_PATH)\")**\" était en cours, nous pouvions observer son état d'avancement :\n",
    "\n",
    "<center><img src=\"./img/EMR_jupyterhub_avancement.png\" width=\"1200\" /></center>\n",
    "\n",
    "Le **serveur d'historique Spark** nous permet une vision beaucoup plus précise de l'exécution des différentes tâche sur les différentes machines du cluster :\n",
    "\n",
    "<center><img src=\"./img/EMR_SHSpark.png\" width=\"1200\" /></center>\n",
    "\n",
    "On peut également constater que notre cluster, comprenant un driver et 2 noeuds principaux, a mis **9 minutes 30** pour traiter les **22 688 images** (8 min pour la réduction de dimension).  \n",
    "*NB : => La réduction de dimension est clairement le facteur limitant de notre solution.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf4e71f-c3da-49b9-b777-09d63a3fc97a",
   "metadata": {},
   "source": [
    "## 3.2 Test de mise à l'échelle horizontale\n",
    "\n",
    "En ajoutant un noeud principal supplémentaire, on observe que le temps de calcul est réduit d'environ 30 % (pour ~33 % de puissance supplémentaire) et passe à **6 min 50**.\n",
    "\n",
    "<center><img src=\"./img/EMR_SHSpark_échelleH.png\" width=\"1200\" /></center>\n",
    "\n",
    "**=> Le test de mise à l'échelle est concluant.**\n",
    "\n",
    "*NB : **Mise à l’échelle :***  \n",
    "***- horizontale :** La mise à l’échelle horizontale est l'ajout de machines supplémentaires au groupe de ressources existant.*  \n",
    "***- verticale :** La mise à l’échelle verticale est l'ajout de plus de puissance de calcul comme le remplacement du processeur et l'ajout de mémoire vive à une machine existante.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d5f44f",
   "metadata": {},
   "source": [
    "# 4. Résiliation de l'instance EMR\n",
    "\n",
    "Notre travail est maintenant terminé.  \n",
    "Le cluster de machines EMR est **facturé à la demande** et nous continuons d'être facturé même lorsque les machines sont au repos. Pour **optimiser la facturation**, il nous faut maintenant **résilier le cluster**.\n",
    "\n",
    "Instructions :\n",
    "1. Commencez par **désactiver le tunnel ssh dans FoxyProxy** pour éviter des problèmes de **timeout**.\n",
    "\n",
    "<center><img src=\"./img/EMR_foxyproxy_desactivation.png\" width=\"400\" /></center>\n",
    "\n",
    "2. Cliquez sur \"**Résilier**\".\n",
    "\n",
    "<center><img src=\"./img/EMR_resiliation_01.png\" width=\"1200\" /></center>\n",
    "\n",
    "3. Confirmez la résiliation.\n",
    "\n",
    "<center><img src=\"./img/EMR_resiliation_02.png\" width=\"675\" /></center>\n",
    "\n",
    "4. La résiliation dure environ **1 minute** avant de se terminer.\n",
    "\n",
    "<center><img src=\"./img/EMR_resiliation_03.png\" width=\"1200\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45053b6",
   "metadata": {},
   "source": [
    "# 5. Cloner le serveur EMR (si besoin)\n",
    "\n",
    "S'il est nécessaire d'exécuter à nouveau le notebook dans les mêmes conditions, il nous suffit de **cloner notre cluster** et ainsi en obtenir une copie fonctionnelle sous 7/20 minutes, le temps de son instanciation.\n",
    "\n",
    "Il est possible de cloner un cluster de 2 manières :\n",
    "- Depuis l'interface AWS cliquez sur \"**Cloner**\".\n",
    "\n",
    "<center><img src=\"./img/EMR_cloner_01.png\" width=\"1200\" /></center>\n",
    "\n",
    "  Vous êtes redirigé vers le même menu de configuration que lors de la création du cluster.  \n",
    "  Cependant, cette fois-ci, tous les champs sont pré-configurés à l'identique de l'état du cluster cloné lors de sa résiliation.  \n",
    "  Il est tout à fait possible de revenir sur les différentes étapes de configuration si des modifications sont souhaitées.  \n",
    "  Lorsque tout est prêt, cliquez sur \"**Cloner un cluster**\".\n",
    "    \n",
    "- En ligne de commande avec AWS CLI d'installé et configuré (en s'assurant de s'attribuer les droits nécessaires sur le compte AMI utilisé).\n",
    "  \n",
    "1. Cliquez sur \"**Exporter AWS CLI**\".\n",
    "\n",
    "<center><img src=\"./img/EMR_cloner_cli_01.png\" width=\"1200\" /></center>\n",
    "    \n",
    "2. Copier/Coller la commande **depuis un terminal**.\n",
    "    \n",
    "<center><img src=\"./img/EMR_cloner_cli_02.png\" width=\"675\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e339b2",
   "metadata": {},
   "source": [
    "# 6. Arborescence du serveur S3 à la fin du projet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d866be0",
   "metadata": {},
   "source": [
    "<center><img src=\"./img/ArborescenceS3-Racine.png\" width=\"675\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eba46f9",
   "metadata": {},
   "source": [
    "# 7. Conclusion\n",
    "\n",
    "A la suite des résultats positifs des tests réalisés en local, cette seconde partie a consisté à créer un **réel cluster de calculs** sur le cloud et d'y exécuter notre solution. <br />\n",
    "\n",
    "L'objectif était de pouvoir **anticiper une future augmentation de la charge de travail**.\n",
    "\n",
    "Le meilleur choix retenu a été l'utilisation du prestataire de services **Amazon Web Services (AWS)** qui nous permet de **louer à la demande de la puissance de calculs**, pour un **coût tout à fait acceptable** via le service **EC2**. Il se classe parmi les offres de type **Infrastructure As A Service (IAAS)**.\n",
    "\n",
    "Cependant, nous sommes allez plus loin en utilisant un service de plus haut niveau de type **Plateforme As A Service (PAAS)** grâce au service **EMR** qui nous permet, en une fois, d'**instancier plusieurs serveur (un cluster)** sur lesquels plusieurs programmes et librairies nécessaires à notre projet on pu être installés et configurés tel que : **Spark**, **Hadoop**, **JupyterHub** ainsi que la librairie **TensorFlow**.\n",
    "\n",
    "En plus d'être plus **rapide et efficace à mettre en place**, les paquets de ces applications sont maintenus à jour et compatibles par les ingénieurs Amazon. Cette validation préalable, a permis d'installer, sans difficulté, **les paquets nécessaires sur l'ensemble des machines du cluster**.\n",
    "\n",
    "Enfin, avec peu de modifications, le notebook a pu être exécuté de la même façon qu'en local, mais cette fois-ci, sur **l'ensemble des images de notre jeu de données**.\n",
    "\n",
    "Le service **Amazon S3** a été sélectionné pour **stocker les données de notre projet**.  \n",
    "S3 offre, pour un faible coût, toutes les conditions dont nous avons besoin pour stocker et exploiter de manière efficace nos données.  \n",
    "*NB : L'espace alloué est potentiellement **illimité**, mais les coûts seront fonction de l'espace utilisé.*\n",
    "\n",
    "Enfin, ce projet a montré pour la suite qu'il est capable de **facilement faire face à une monté de la charge de travail** en **redimensionnant** simplement notre cluster de machines (horizontalement et/ou verticalement au besoin). Les coûts augmenteront en conséquence mais resteront nettement inférieurs aux coûts qu'engendrerait l'achat de matériels ou la location de serveurs dédiés."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "720px",
    "width": "468px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "308.625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
