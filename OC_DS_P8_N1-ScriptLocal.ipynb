{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "441159cc",
   "metadata": {},
   "source": [
    "# Déployer un modèle dans le cloud\n",
    "# Notebook 1 - Compléter l'application et la tester en local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7668a4",
   "metadata": {},
   "source": [
    "# Objectifs du notebook\n",
    "\n",
    "Reprendre et compléter le travail de développement réalisé par le stagiaire en local avant déploiement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d658fb6",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Déployer-un-modèle-dans-le-cloud\" data-toc-modified-id=\"Déployer-un-modèle-dans-le-cloud-1\">Déployer un modèle dans le cloud</a></span></li><li><span><a href=\"#Notebook-1---Compléter-l'application-et-la-tester-en-local\" data-toc-modified-id=\"Notebook-1---Compléter-l'application-et-la-tester-en-local-2\">Notebook 1 - Compléter l'application et la tester en local</a></span></li><li><span><a href=\"#Objectifs-du-notebook\" data-toc-modified-id=\"Objectifs-du-notebook-3\">Objectifs du notebook</a></span></li><li><span><a href=\"#1.-Préambule\" data-toc-modified-id=\"1.-Préambule-4\">1. Préambule</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.1-Problématique\" data-toc-modified-id=\"1.1-Problématique-4.1\">1.1 Problématique</a></span></li><li><span><a href=\"#1.2-Objectifs-dans-ce-projet\" data-toc-modified-id=\"1.2-Objectifs-dans-ce-projet-4.2\">1.2 Objectifs dans ce projet</a></span></li><li><span><a href=\"#1.3-Déroulement-des-étapes-du-projet\" data-toc-modified-id=\"1.3-Déroulement-des-étapes-du-projet-4.3\">1.3 Déroulement des étapes du projet</a></span></li></ul></li><li><span><a href=\"#2.-Choix-techniques-généraux-retenus\" data-toc-modified-id=\"2.-Choix-techniques-généraux-retenus-5\">2. Choix techniques généraux retenus</a></span><ul class=\"toc-item\"><li><span><a href=\"#2.1-Calcul-distribué\" data-toc-modified-id=\"2.1-Calcul-distribué-5.1\">2.1 Calcul distribué</a></span></li><li><span><a href=\"#2.2-Transfert-Learning\" data-toc-modified-id=\"2.2-Transfert-Learning-5.2\">2.2 Transfert Learning</a></span></li></ul></li><li><span><a href=\"#3.-Déploiement-de-la-solution-en-local\" data-toc-modified-id=\"3.-Déploiement-de-la-solution-en-local-6\">3. Déploiement de la solution en local</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.1-Environnement-de-travail\" data-toc-modified-id=\"3.1-Environnement-de-travail-6.1\">3.1 Environnement de travail</a></span></li><li><span><a href=\"#3.2-Installation-de-Spark\" data-toc-modified-id=\"3.2-Installation-de-Spark-6.2\">3.2 Installation de Spark</a></span></li><li><span><a href=\"#3.3-Installation-des-paquets\" data-toc-modified-id=\"3.3-Installation-des-paquets-6.3\">3.3 Installation des paquets</a></span></li><li><span><a href=\"#3.4-Configuration-graphique-du-notebook\" data-toc-modified-id=\"3.4-Configuration-graphique-du-notebook-6.4\">3.4 Configuration graphique du notebook</a></span></li><li><span><a href=\"#3.5-Import-des-blibliothèques\" data-toc-modified-id=\"3.5-Import-des-blibliothèques-6.5\">3.5 Import des blibliothèques</a></span></li><li><span><a href=\"#3.6-Définition-des-PATH-pour-charger-les-images--et-enregistrer-les-résultats\" data-toc-modified-id=\"3.6-Définition-des-PATH-pour-charger-les-images--et-enregistrer-les-résultats-6.6\">3.6 Définition des PATH pour charger les images <br> et enregistrer les résultats</a></span></li><li><span><a href=\"#3.7-Création-de-la-session-Spark\" data-toc-modified-id=\"3.7-Création-de-la-session-Spark-6.7\">3.7 Création de la session Spark</a></span></li><li><span><a href=\"#3.8-Traitement-des-données\" data-toc-modified-id=\"3.8-Traitement-des-données-6.8\">3.8 Traitement des données</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.8.1-Chargement-des-données\" data-toc-modified-id=\"3.8.1-Chargement-des-données-6.8.1\">3.8.1 Chargement des données</a></span></li><li><span><a href=\"#3.8.2-Préparation-du-modèle\" data-toc-modified-id=\"3.8.2-Préparation-du-modèle-6.8.2\">3.8.2 Préparation du modèle</a></span></li><li><span><a href=\"#3.8.3-Définition-du-processus-de-chargement-des-images-et-application-de-leur-featurisation-à-travers-l'utilisation-de-pandas-UDF\" data-toc-modified-id=\"3.8.3-Définition-du-processus-de-chargement-des-images-et-application-de-leur-featurisation-à-travers-l'utilisation-de-pandas-UDF-6.8.3\">3.8.3 Définition du processus de chargement des images et application <br>de leur featurisation à travers l'utilisation de pandas UDF</a></span></li><li><span><a href=\"#3.8.4-Exécution-des-actions-d'extraction-de-features\" data-toc-modified-id=\"3.8.4-Exécution-des-actions-d'extraction-de-features-6.8.4\">3.8.4 Exécution des actions d'extraction de features</a></span></li><li><span><a href=\"#3.8.5-Ajout-d'une-ACP-pour-optimiser-les-performances\" data-toc-modified-id=\"3.8.5-Ajout-d'une-ACP-pour-optimiser-les-performances-6.8.5\">3.8.5 Ajout d'une ACP pour optimiser les performances</a></span><ul class=\"toc-item\"><li><span><a href=\"#3.8.5.1-Chargement-des-bibliothèques-nécessaires\" data-toc-modified-id=\"3.8.5.1-Chargement-des-bibliothèques-nécessaires-6.8.5.1\">3.8.5.1 Chargement des bibliothèques nécessaires</a></span></li><li><span><a href=\"#3.8.5.2-Préparer-les-données-à-leurs-manipulation-par-pySpark\" data-toc-modified-id=\"3.8.5.2-Préparer-les-données-à-leurs-manipulation-par-pySpark-6.8.5.2\">3.8.5.2 Préparer les données à leurs manipulation par pySpark</a></span></li><li><span><a href=\"#3.8.5.3-Détermination-du-nombre-minimal-de-composantes-requises-pour-expliquer-plus-de-95-%-de-la-variance\" data-toc-modified-id=\"3.8.5.3-Détermination-du-nombre-minimal-de-composantes-requises-pour-expliquer-plus-de-95-%-de-la-variance-6.8.5.3\">3.8.5.3 Détermination du nombre minimal de composantes requises pour expliquer plus de 95 % de la variance</a></span></li><li><span><a href=\"#3.8.5.4-Application-de-l'ACP-sur-le-nombre-de-composants-principaux-déterminés-précédemment\" data-toc-modified-id=\"3.8.5.4-Application-de-l'ACP-sur-le-nombre-de-composants-principaux-déterminés-précédemment-6.8.5.4\">3.8.5.4 Application de l'ACP sur le nombre de composants principaux déterminés précédemment</a></span></li></ul></li><li><span><a href=\"#3.8.6--Exportation-des-features-au-format-parquet\" data-toc-modified-id=\"3.8.6--Exportation-des-features-au-format-parquet-6.8.6\">3.8.6  Exportation des features au format parquet</a></span></li></ul></li><li><span><a href=\"#3.9-Chargement-des-données-enregistrées-et-validation-du-résultat\" data-toc-modified-id=\"3.9-Chargement-des-données-enregistrées-et-validation-du-résultat-6.9\">3.9 Chargement des données enregistrées et validation du résultat</a></span></li><li><span><a href=\"#3.10-Exportation-du-dataframe-des-transformations-des-images\" data-toc-modified-id=\"3.10-Exportation-du-dataframe-des-transformations-des-images-6.10\">3.10 Exportation du dataframe des transformations des images</a></span></li></ul></li><li><span><a href=\"#4.-Conclusion\" data-toc-modified-id=\"4.-Conclusion-7\">4. Conclusion</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2cee08",
   "metadata": {},
   "source": [
    "# 1. Préambule\n",
    "\n",
    "## 1.1 Problématique\n",
    "\n",
    "La très jeune start-up de l'AgriTech, nommée \"**Fruits**!\", <br />\n",
    "cherche à proposer des solutions innovantes pour la récolte des fruits.\n",
    "\n",
    "La volonté de l’entreprise est de préserver la biodiversité des fruits <br />\n",
    "en permettant des traitements spécifiques pour chaque espèce de fruits <br />\n",
    "en développant des robots cueilleurs intelligents.\n",
    "\n",
    "La start-up souhaite dans un premier temps se faire connaître en mettant <br />\n",
    "à disposition du grand public une application mobile qui permettrait aux <br />\n",
    "utilisateurs de prendre en photo un fruit et d'obtenir des informations sur ce fruit.\n",
    "\n",
    "Pour la start-up, cette application permettrait de sensibiliser le grand public <br /> \n",
    "à la biodiversité des fruits et de mettre en place une première version du moteur <br />\n",
    "de classification des images de fruits.\n",
    "\n",
    "De plus, le développement de l’application mobile permettra de construire <br />\n",
    "une première version de l'architecture **Big Data** nécessaire.\n",
    "\n",
    "## 1.2 Objectifs dans ce projet\n",
    "\n",
    "1. Développer une première chaîne de traitement des données qui <br />\n",
    "   comprendra le **preprocessing** et une étape de **réduction de dimension**.\n",
    "2. Tenir compte du fait que <u>le volume de données va augmenter <br />\n",
    "   très rapidement</u> après la livraison de ce projet, ce qui implique de:\n",
    " - Déployer le traitement des données dans un environnement **Big Data**\n",
    " - Développer les scripts en **pyspark** pour effectuer du **calcul distribué**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b95e6ce",
   "metadata": {},
   "source": [
    "## 1.3 Déroulement des étapes du projet\n",
    "\n",
    "Le projet va être réalisé en 2 temps, dans deux environnements différents. <br />\n",
    "Nous allons dans un premier temps développer et exécuter notre code en local, <br />\n",
    "en travaillant sur un nombre limité d'images à traiter.\n",
    "\n",
    "Une fois les choix techniques validés, nous déploierons notre solution <br />\n",
    "dans un environnement Big Data en mode distribué.\n",
    "\n",
    "<u>Pour cette raison, ce projet sera divisé en 2 parties</u>:\n",
    "1. Liste des choix techniques généraux retenus\n",
    "2. Déploiement de la solution en local\n",
    "3. Déploiement de la solution dans le cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b34029",
   "metadata": {},
   "source": [
    "# 2. Choix techniques généraux retenus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32baf092",
   "metadata": {},
   "source": [
    "## 2.1 Calcul distribué\n",
    "\n",
    "L’énoncé du projet nous impose de développer des scripts en **pyspark** <br />\n",
    "afin de <u>prendre en compte l’augmentation très rapide du volume <br />\n",
    "de données après la livraison du projet</u>.\n",
    "\n",
    "Pour comprendre rapidement et simplement ce qu’est **pyspark** <br />\n",
    "et son principe de fonctionnement : [PySpark : Tout savoir sur la librairie Python](https://datascientest.com/pyspark)\n",
    "\n",
    "<u>Le début de l’article nous dit ceci </u>:<br />\n",
    "« *Lorsque l’on parle de traitement de bases de données sur python, <br />\n",
    "on pense immédiatement à la librairie pandas. Cependant, lorsqu’on a <br />\n",
    "à faire à des bases de données trop massives, les calculs deviennent trop lents.<br />\n",
    "Heureusement, il existe une autre librairie python, assez proche <br />\n",
    "de pandas, qui permet de traiter de très grandes quantités de données : PySpark.<br />\n",
    "Apache Spark est un framework open-source développé par l’AMPLab <br />\n",
    "de UC Berkeley permettant de traiter des bases de données massives <br />\n",
    "en utilisant le calcul distribué, technique qui consiste à exploiter <br />\n",
    "plusieurs unités de calcul réparties en clusters au profit d’un seul <br />\n",
    "projet afin de diviser le temps d’exécution d’une requête.<br />\n",
    "Spark a été développé en Scala et est au meilleur de ses capacités <br />\n",
    "dans son langage natif. Cependant, la librairie PySpark propose de <br />\n",
    "l’utiliser avec le langage Python, en gardant des performances <br />\n",
    "similaires à des implémentations en Scala.<br />\n",
    "Pyspark est donc une bonne alternative à la librairie pandas lorsqu’on <br />\n",
    "cherche à traiter des jeux de données trop volumineux qui entraînent <br />\n",
    "des calculs trop chronophages.* »\n",
    "\n",
    "Comme nous le constatons, **pySpark** est un moyen de communiquer <br />\n",
    "avec **Spark** via le langage **Python**.<br />\n",
    "**Spark**, quant à lui, est un outil qui permet de gérer et de coordonner <br />\n",
    "l'exécution de tâches sur des données à travers un groupe d'ordinateurs. <br />\n",
    "<u>Spark (ou Apache Spark) est un framework open source de calcul distribué <br />\n",
    "in-memory pour le traitement et l'analyse de données massives</u>.\n",
    "\n",
    "Un autre [article très intéressant et beaucoup plus complet pour <br />\n",
    "comprendre le **fonctionnement de Spark**](https://www.veonum.com/apache-spark-pour-les-nuls/), ainsi que le rôle <br />\n",
    "des **Spark Session** que nous utiliserons dans ce projet.\n",
    "\n",
    "<u>Voici également un extrait</u>:\n",
    "\n",
    "*Les applications Spark se composent d’un pilote (« driver process ») <br />\n",
    "et de plusieurs exécuteurs (« executor processes »). Il peut être configuré <br />\n",
    "pour être lui-même l’exécuteur (local mode) ou en utiliser autant que <br />\n",
    "nécessaire pour traiter l’application, Spark prenant en charge la mise <br />\n",
    "à l’échelle automatique par une configuration d’un nombre minimum <br />\n",
    "et maximum d’exécuteurs.*\n",
    "\n",
    "![Schéma de Spark](img/spark-schema.png)\n",
    "\n",
    "*Le driver (parfois appelé « Spark Session ») distribue et planifie <br />\n",
    "les tâches entre les différents exécuteurs qui les exécutent et permettent <br />\n",
    "un traitement réparti. Il est le responsable de l’exécution du code <br />\n",
    "sur les différentes machines.\n",
    "\n",
    "Chaque exécuteur est un processus Java Virtual Machine (JVM) distinct <br />\n",
    "dont il est possible de configurer le nombre de CPU et la quantité de <br />\n",
    "mémoire qui lui est alloué. <br />\n",
    "Une seule tâche peut traiter un fractionnement de données à la fois.*\n",
    "\n",
    "Dans les deux environnements (Local et Cloud) nous utiliserons donc **Spark** <br />\n",
    "et nous l’exploiterons à travers des scripts python grâce à **PySpark**.\n",
    "\n",
    "Dans la <u>version locale</u> de notre script nous **simulerons <br />\n",
    "le calcul distribué** afin de valider que notre solution fonctionne.<br />\n",
    "Dans la <u>version cloud</u> nous **réaliserons les opérations sur un cluster de machine**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5364c9f9",
   "metadata": {},
   "source": [
    "## 2.2 Transfert Learning\n",
    "\n",
    "Il est également demandé de <br />\n",
    "réaliser une première chaîne de traitement <br />\n",
    "des données qui comprendra le preprocessing et <br />\n",
    "une étape de réduction de dimension.\n",
    "\n",
    "Il est aussi précisé qu'il n'est pas nécessaire <br />\n",
    "d'entraîner un modèle pour le moment.\n",
    "\n",
    "Nous décidons de partir sur une solution de **transfert learning**.\n",
    "\n",
    "Simplement, le **transfert learning** consiste <br />\n",
    "à utiliser l'expérience déjà accumulée <br />\n",
    "par un modèle entraîné (ici **MobileNetV2**) pour <br />\n",
    "l'adapter à notre problématique.\n",
    "\n",
    "Nous allons fournir au modèle nos images, et nous allons <br />\n",
    "<u>récupérer l'avant dernière couche</u> du modèle.<br />\n",
    "En effet, la dernière couche est une couche softmax <br />\n",
    "qui permet la classification des images ce que nous ne <br />\n",
    "souhaitons pas dans ce projet.\n",
    "\n",
    "L'avant dernière couche correspond à un **vecteur <br />\n",
    "réduit** (de dimension (1,1,1280) dans le cas du modèle sélectionné dans ce notebook).\n",
    "\n",
    "Cela permettra de réaliser une première version du moteur <br />\n",
    "pour la classification de notre échantillon d'images de fruits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e89a2da",
   "metadata": {},
   "source": [
    "# 3. Déploiement de la solution en local\n",
    "\n",
    "\n",
    "## 3.1 Environnement de travail\n",
    "\n",
    "Pour des raisons de simplicité, nous développons dans un environnement <br />\n",
    "Linux Unbuntu (exécuté depuis une machine Windows dans une machine virtuelle)\n",
    "* Pour installer une machine virtuelle :  https://www.malekal.com/meilleurs-logiciels-de-machine-virtuelle-gratuits-ou-payants/\n",
    "\n",
    "## 3.2 Installation de Spark\n",
    "\n",
    "[La première étape consiste à installer Spark ](https://computingforgeeks.com/how-to-install-apache-spark-on-ubuntu-debian/)\n",
    "\n",
    "## 3.3 Installation des paquets\n",
    "\n",
    "<u>On installe ensuite à l'aide de la commande **pip** <br />\n",
    "les paquets qui nous seront nécessaires</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "728d9256",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install pandas pillow tensorflow pyspark pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25541b9c",
   "metadata": {},
   "source": [
    "*NB : Finalement, ces paquets ont directement été installé dans l'environnement anaconda utilisé pour compléter ce notebook.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3c53c7",
   "metadata": {},
   "source": [
    "## 3.4 Configuration graphique du notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47fae17a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set the width of the notebook sheet to the width of the window.\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a43845",
   "metadata": {},
   "source": [
    "## 3.5 Import des blibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5c0c74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 23:17:29.887408: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.22 s, sys: 366 ms, total: 1.59 s\n",
      "Wall time: 1.77 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Import base libraries for data manipulations.\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "\n",
    "# Import libraries relative to the preprocessing steps.\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# Import required pySpark libraries.\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "# Notebook's warning management.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6c7562",
   "metadata": {},
   "source": [
    "*NB: Explication du message d'information de TensorFlow : https://stackoverflow.com/questions/65298241/what-does-this-tensorflow-message-mean-any-side-effect-was-the-installation-su*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661ff67c",
   "metadata": {},
   "source": [
    "## 3.6 Définition des PATH pour charger les images <br /> et enregistrer les résultats\n",
    "\n",
    "Dans cette version les données sont stockées dans des répertoires locaux.<br />\n",
    "Seulement un échantillon de **393 images** est utilisé pour les tests de fonctionnement du code <br />\n",
    "dans cette première version en local. L'échantillon d'images à charger est stocké <br />\n",
    "dans le dossier **test_local_sample**.<br />\n",
    "\n",
    "Les résultats des différents traitements appliqués seront stockés  <br />\n",
    "dans le dossier **local_results**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91ddb627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the absolute current folder path of the notebook.\n",
    "PROJ_PATH = os.getcwd()\n",
    "    \n",
    "# Get the path of the datasets directory.\n",
    "#DATASETS_DIR_PATH = os.path.join(PROJ_PATH, \"datasets\")\n",
    "    \n",
    "# Path of the folder of the test sample for local use.\n",
    "LOCAL_FOLDER_NAME = \"test_local_sample\"\n",
    "LOCAL_SP_DATASET_PATH = os.path.join(PROJ_PATH, \"datasets\", LOCAL_FOLDER_NAME)\n",
    "\n",
    "# Result folder path. \n",
    "RESULT_PATH = os.path.join(PROJ_PATH, \"datasets\", \"local_results\")\n",
    "\n",
    "# Check paths.\n",
    "print('Project folder path:       ' + PROJ_PATH +\n",
    "      '\\nDataset folder path:       ' +  LOCAL_SP_DATASET_PATH +\n",
    "      '\\nLocal results folder path: ' + RESULT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5e637a",
   "metadata": {},
   "source": [
    "## 3.7 Création de la session Spark\n",
    "\n",
    "L’application Spark est contrôlée grâce à un processus de pilotage (driver process) appelé **SparkSession**. <br />\n",
    "<u>Une instance de **SparkSession** est la façon dont Spark exécute les fonctions définies par l’utilisateur <br />\n",
    "dans l’ensemble du cluster</u>. <u>Une SparkSession correspond toujours à une application Spark</u>.\n",
    "\n",
    "<u>Ici nous créons une session spark en spécifiant dans l'ordre</u> :\n",
    " 1. un **nom pour l'application**, qui sera affichée dans l'interface utilisateur Web Spark \"**P8**\"\n",
    " 2. que l'application doit s'exécuter **localement**. <br />\n",
    "   *NB : Le nombre de cœurs à utiliser n'a pas besoin d'être défini (par exemple, .master('local[4]) pour 4 cœurs à utiliser) car <br />\n",
    "   tous les cœurs disponibles dans le processeur sont noarmalement utilisés par défaut, mais pour être sûr, ce sera indiqué dans le code : => local[\\*]. (NB : local[1] <=> local).*<br />\n",
    " 3. une option de configuration supplémentaire permettant d'utiliser le **format \"parquet\"** <br />\n",
    "   que nous utiliserons pour enregistrer et charger le résultat de notre travail.\n",
    " 4. vouloir **obtenir une session spark** existante ou si aucune n'existe, en créer une nouvelle\n",
    " \n",
    "*NB : Conseils sur l'optimisation du nombre d'exécuteurs, de CPU coeur et de la mémoire : https://spoddutur.github.io/spark-notes/distribution_of_executors_cores_and_memory_for_spark_application.html* </br>\n",
    "=> D'après ces conseils, dans notre cas d'utilisation (AWS m5.xlarge : 16 GiB RAM et 4 cores CPU par instance), </br>\n",
    "nous aurons seulement 4 coeurs disponibles par node (m5.xlarge) < 5.</br>\n",
    "   => Un exécuteur par node sera donc suffisant (=> 4 coeurs et 16 GiB par exécuteur)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7bea157",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Create or get the Spark instance.\n",
    "spark = (SparkSession.builder\n",
    "                     .appName('P8')\n",
    "                     .master('local[*]')\n",
    "                     .config(\"spark.sql.parquet.writeLegacyFormat\", 'true')\n",
    "                     #.config(\"spark.executor.instances\", \"1\")\n",
    "                     #.config(\"spark.executor.cores\", \"4\")\n",
    "                     #.config(\"spark.default.parallelism\", \"4\")\n",
    "                     #.config(\"spark.driver.memory\", \"2G\")\n",
    "                     #.config(\"spark.dynamicAllocation.enabled\", 'true')\n",
    "                     #.config(\"spark.dynamicAllocation.minExecutors\",\"1\")\n",
    "                     #.config(\"spark.dynamicAllocation.maxExecutors\",\"2\")\n",
    "                     #.config(\"spark.sql.autoBroadcastJoinThreshold\", '-1')\n",
    "                     .getOrCreate()\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8b53ac",
   "metadata": {},
   "source": [
    "<u>Nous créons également la variable \"**sc**\" qui est un **SparkContext** issue de la variable **spark**</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14aeccb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For convinience, create a SparkContext variable.\n",
    "sc = spark.sparkContext\n",
    "#sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Set the warning level to display in the notebook.\n",
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a086010",
   "metadata": {},
   "source": [
    "<u>Affichage des informations de Spark en cours d'execution</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68313059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check SparkSession config.\n",
    "print(\"   --- SparkSession config ---\")\n",
    "spark.sparkContext._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e97bf13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the spark UI link.\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195a88b0",
   "metadata": {},
   "source": [
    "## 3.8 Traitement des données\n",
    "\n",
    "<u>Dans la suite de notre flux de travail, <br />\n",
    "nous allons successivement</u> :\n",
    "1. Préparer nos données\n",
    "    1. Importer les images dans un dataframe **pandas UDF**\n",
    "    2. Associer aux images leur **label**\n",
    "    3. Préprocesser en **redimensionnant nos images pour <br />\n",
    "       qu'elles soient compatibles avec notre modèle**\n",
    "2. Préparer notre modèle\n",
    "    1. Importer le modèle **MobileNetV2**\n",
    "    2. Créer un **nouveau modèle** dépourvu de la dernière couche de MobileNetV2\n",
    "3. Définir le processus de chargement des images et l'application <br />\n",
    "   de leur featurisation à travers l'utilisation de pandas UDF\n",
    "3. Exécuter les actions d'extraction de features\n",
    "4. Enregistrer le résultat de nos actions\n",
    "5. Tester le bon fonctionnement en chargeant les données enregistrées\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386fe0bc",
   "metadata": {},
   "source": [
    "### 3.8.1 Chargement des données\n",
    "\n",
    "Les images sont chargées au format binaire, ce qui offre, <br />\n",
    "plus de souplesse dans la façon de prétraiter les images.\n",
    "\n",
    "Avant de charger les images, nous spécifions que nous voulons charger <br />\n",
    "uniquement les fichiers dont l'extension est **jpg**.\n",
    "\n",
    "Nous indiquons également de charger tous les objets possibles contenus <br />\n",
    "dans les sous-dossiers du dossier communiqué."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "512b46c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow messages management.\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # Filter: NOTHING '0', INFO '1', WARNING '2', ERROR '3'\n",
    "\n",
    "# Import the sample of images.\n",
    "images = spark.read.format(\"binaryFile\")\\\n",
    "                   .option(\"pathGlobFilter\", \"*.jpg\")\\\n",
    "                   .option(\"recursiveFileLookup\", \"true\")\\\n",
    "                   .load(LOCAL_SP_DATASET_PATH)\n",
    "\n",
    "# NB: Duplicated and commented code as comments cannot be used in front of each lines.\n",
    "#images = spark.read.format(\"binaryFile\") \\ # Store loaded images as a binary format.\n",
    "#                   .option(\"pathGlobFilter\", \"*.jpg\") \\ # Get all and only jpg files...\n",
    "#                   .option(\"recursiveFileLookup\", \"true\") \\ # ... from any folder...\n",
    "#                   .load(LOCAL_SP_DATASET_PATH) # ... within the root one. #os.path.join(LOCAL_SP_DATASET_PATH, \"Apple Braeburn\"))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645faeaf",
   "metadata": {},
   "source": [
    "<u>Affichage des 5 premières images contenant</u> :\n",
    " - le path de l'image\n",
    " - la date et heure de sa dernière modification\n",
    " - sa longueur\n",
    " - son contenu encodé en valeur hexadécimal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863981e5",
   "metadata": {},
   "source": [
    "<u>Je ne conserve que le **path** de l'image et j'ajoute <br />\n",
    "    une colonne contenant les **labels** de chaque image</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b130143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the raw image pySpark dataframe schema.\n",
    "print(\"   --- Image pySpark dataframe schema ---\")\n",
    "images.printSchema()\n",
    "\n",
    "# Show an abstract of the raw image pySpark dataframe.\n",
    "images.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95431e90",
   "metadata": {},
   "source": [
    "**Conseils pour déterminer le nombre optimal de partitions à utiliser :** https://stackoverflow.com/questions/64600212/how-to-determine-the-partition-size-in-an-apache-spark-dataframe\n",
    "- 3 à 4 fois le nombre de coeurs de processeurs total disponibles.\n",
    "- Eviter les partitions supérieures à 1 GB. Méthode empirique commune : 100 MB < par partition < 1 GB.\n",
    "\n",
    "Dans notre cas la colonne \"preprocessed\" du dataframe sdf (le dataframe suivant dans le notebook) contient </br>\n",
    "des vecteurs à 150 000 dimensions. Alors que les autres colonnes contiennent </br>\n",
    "des vecteurs allant de 208 à 7 000 dimensions.</br>\n",
    "\n",
    "=> Ainsi, la colonne preprocessed définit à elle seule la taille en mémoire que prend le dataframe spark </br>\n",
    "(240 MiB pour 393 images). Par souci d'optimisation, </br>\n",
    "cette colonne ne ce sera pas ajoutée au dataframe dans le notebook destiné à être éxecuté dans le cloud </br>\n",
    "(Elle est conservée ici pour mieux mettre en évidence les différentes étapes de traitements des images sur notre échantillon). </br>\n",
    "\n",
    "Passant de 240 MiB à 6.5 MiB pour 393 images, il est possible d'estimer que le dataset à traiter fera : </br>\n",
    "**Taille mémoire du dataset échantillon / n_images_ech * n_images_dataset = 6.5/393 * 22 688 =~ 375 MiB.**</br>\n",
    "\n",
    "   => Chaque partition sera donc forcément infèrieure à 1 GiB. </br>\n",
    "   Par conséquent, l'empreinte mémoire n'a pas d'influence sur nos choix de partitionnement.\n",
    "   \n",
    "*NB: Le nombre d'images étant appelé à augmenter rapidement, les partitions qui seront infèrieures à 100 MB, compte tenu du dataset à disposition dans ce projet, devraient par la suite finir par dépasser cette limite.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d57c1cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the number of partitions in which split the dataframe.\n",
    "# NB: sc.defaultParallelism returns the number of processor cores available within the whole cluster.\n",
    "images = images.coalesce(sc.defaultParallelism * 3) \n",
    "\n",
    "# Check if the number of partition fits the number of processor cores.\n",
    "print(\"There are %i processor cores available within the whole cluster of processing instances.\" % sc.defaultParallelism)\n",
    "print(\"The spark dataframe is splitted in %i partitions.\" % images.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a08b0494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the image labels column.\n",
    "images = images.withColumn('label', element_at(split(images['path'], '/'), -2))\n",
    "\n",
    "print(\"   --- Image new schema ---\")\n",
    "print(images.printSchema())\n",
    "print()\n",
    "print(\"   --- Check values of the new column: label ---\")\n",
    "print(images.select('path', 'label').show(5, False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d47705",
   "metadata": {},
   "source": [
    "### 3.8.2 Préparation du modèle\n",
    "\n",
    "La technique du **transfert learning** est utilisée pour extraire les features des images.<br />\n",
    "**MobileNetV2** est le modèle retenu pour sa <u>rapidité d'exécution</u>, <br />\n",
    "particulièrement adaptée au traitement d'un gros volume de données, <br />\n",
    "et pour la <u>faible dimensionnalité du vecteur de caractéristique qu'il délivre en sortie</u> : (1,1,1280).\n",
    "\n",
    "Pour en savoir plus sur la conception et le fonctionnement de MobileNetV2, <br />\n",
    "je vous invite à lire [cet article](https://towardsdatascience.com/review-mobilenetv2-light-weight-model-image-classification-8febb490e61c).\n",
    "\n",
    "<u>Voici le schéma de son architecture globale</u> : \n",
    "\n",
    "![Architecture de MobileNetV2](img/mobilenetv2_architecture.png)\n",
    "\n",
    "Il existe une dernière couche qui sert à classer les images <br />\n",
    "selon 1000 catégories que nous ne voulons pas utiliser.<br />\n",
    "L'idée dans ce projet est de récupérer le **vecteur de caractéristiques** <br />\n",
    "qui servira, plus tard, à grâce à un moteur de classification à reconnaitre les différents fruits du jeu de données.\n",
    "\n",
    "Comme d'autres modèles similaires, **MobileNetV2**, lorsqu'on l'utilise <br />\n",
    "en incluant toutes ses couches, attend obligatoirement des images <br />\n",
    "de dimension (224,224,3) en entrée. Nos images étant toutes de dimension (100,100,3), <br />\n",
    "nous devrons simplement les **redimensionner** avant de les confier au modèle.\n",
    "\n",
    "<u>Dans l'odre</u> :\n",
    " 1. Nous chargeons le modèle **MobileNetV2** avec les poids **précalculés** <br />\n",
    "    issus d'**imagenet** et en spécifiant le format de nos images en entrée\n",
    " 2. Nous créons un nouveau modèle avec:\n",
    "  - <u>en entrée</u> : l'entrée du modèle MobileNetV2.\n",
    "  - <u>en sortie</u> : l'avant dernière couche du modèle MobileNetV2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cdd9bdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the model MobileNetV2 with pretrained weights over imagenet.\n",
    "model = MobileNetV2(weights='imagenet',\n",
    "                    include_top=True,\n",
    "                    input_shape=(224, 224, 3)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "99d6b68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new model.\n",
    "new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b197379",
   "metadata": {},
   "source": [
    "Affichage du résumé de notre nouveau modèle où nous constatons <br />\n",
    "que <u>nous récupérons bien en sortie un vecteur de dimension (1, 1, 1280)</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8207725",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show the structure of the model.\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0adcf5",
   "metadata": {},
   "source": [
    "Tous les workeurs doivent pouvoir accéder au modèle ainsi qu'à ses poids. <br />\n",
    "Une bonne pratique consiste à charger le modèle sur le driver puis à diffuser <br />\n",
    "ensuite les poids aux différents workeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cc53ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights from the pretrained model for broadcasting among all workers.\n",
    "broadcast_weights = sc.broadcast(new_model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc0e34e",
   "metadata": {},
   "source": [
    "<u>Mettons cela sous forme de fonction</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fd51ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn ():\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns a MobileNetV2 model with its top layer removed and its pretrained weights broadcasted.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the MobileNetV2 model.\n",
    "    model = MobileNetV2(weights='imagenet',\n",
    "                        include_top=True,\n",
    "                        input_shape=(224, 224, 3))\n",
    "    \n",
    "    # Remove trainability from all layers.\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    # Create the new model without the last classification layer.\n",
    "    new_model = Model(inputs=model.input,\n",
    "                      outputs=model.layers[-2].output)\n",
    "    \n",
    "    # Broadcast the defined weights to the workers.\n",
    "    new_model.set_weights(broadcast_weights.value)\n",
    "    \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5620876",
   "metadata": {},
   "source": [
    "### 3.8.3 Définition du processus de chargement des images et application <br/>de leur featurisation à travers l'utilisation de pandas UDF\n",
    "\n",
    "Ce notebook définit la logique par étapes, jusqu'à Pandas UDF.\n",
    "\n",
    "<u>L'empilement des appels est la suivante</u> :\n",
    "\n",
    "- Pandas UDF\n",
    "  - featuriser une série d'images pd.Series\n",
    "   - prétraiter une image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc4e5f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess (content):\n",
    "    \n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes to make them fit the input layer format requirements of the model.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Image opened in RAM and resized to the required format by the model.\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    \n",
    "    # Convert the image to a numpy array which can be given to the entry points of the model.\n",
    "    arr = img_to_array(img)\n",
    "    \n",
    "    # Normalize (min max scaler) values within the numpy array.\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "\n",
    "def preprocessed (series_of_contents_batch):\n",
    "    \n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes to make them fit the input layer format requirements of the model.\n",
    "    => This function returns the preprocessed images.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Preprocess all images from their byte format to a numpy array format with a resize to 224 x 224.\n",
    "    preproc = np.stack(series_of_contents_batch.map(preprocess))\n",
    "       \n",
    "    # For some layers, output features will be multi-dimensional tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preproc]\n",
    "    \n",
    "    return pd.Series(output)\n",
    "\n",
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def preproc_udf (series_of_contents_all_batches):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function allows to get tall preprocessed images in a column to join to the pySpark dataframe.\n",
    "    NB: This is optional, only to clarify the presentation.\n",
    "                              \n",
    "    \"\"\"\n",
    "       \n",
    "    # Loop over each batch.\n",
    "    for series_of_contents_batch in series_of_contents_all_batches:\n",
    "        \n",
    "        # The function returns all preprocessed images from each batch in a generator.\n",
    "        yield preprocessed(series_of_contents_batch)\n",
    "\n",
    "\n",
    "def featurize_series (model, series_of_contents_batch):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----------\n",
    "    \n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    A pd.Series of image features.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Preprocess all images from their byte format to a numpy array format with a resize to 224 x 224.\n",
    "    input = np.stack(series_of_contents_batch.map(preprocess))\n",
    "    \n",
    "    # Features' extraction over each image.\n",
    "    preds = model.predict(input)\n",
    "    \n",
    "    # For some layers, output features will be multi-dimensional tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    \n",
    "    return pd.Series(output)\n",
    "\n",
    "\n",
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf (series_of_contents_all_batches):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----------\n",
    "    \n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    \n",
    "    NB: The decorator allows the user to create a method that can be applied a dataframe with Spark and\n",
    "    specifies the type of the object returned: here a Spark DataFrame column of type ArrayType(FloatType).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    series_of_contents_all_batches: \n",
    "        This argument is an iterator over batches of data, where each batch is a pandas Series of image data.\n",
    "                              \n",
    "    \"\"\"\n",
    "    \n",
    "    # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "    # for multiple data batches. This amortizes the overhead of loading big models.\n",
    "    model = model_fn()\n",
    "    \n",
    "    # Loop over each batch.\n",
    "    for series_of_contents_batch in series_of_contents_all_batches:\n",
    "        \n",
    "        # The function returns all extracted features from each batch in a generator (a tuple).\n",
    "        yield featurize_series(model, series_of_contents_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdf2ef9",
   "metadata": {},
   "source": [
    "### 3.8.4 Exécution des actions d'extraction de features\n",
    "\n",
    "Les Pandas UDF, sur de grands enregistrements (par exemple, de très grandes images), <br />\n",
    "peuvent rencontrer des erreurs de type Out Of Memory (OOM).<br />\n",
    "Si vous rencontrez de telles erreurs dans la cellule ci-dessous, <br />\n",
    "essayez de réduire la taille du lot Arrow via 'maxRecordsPerBatch'.\n",
    "\n",
    "Je n'utiliserai pas cette commande dans ce projet <br />\n",
    "et je laisse donc la commande en commentaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f30d28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f8f95d",
   "metadata": {},
   "source": [
    "Nous pouvons maintenant exécuter la featurisation sur l'ensemble de notre DataFrame Spark.<br />\n",
    "<u>REMARQUE</u> : Cela peut prendre beaucoup de temps, tout dépend du volume de données à traiter. <br />\n",
    "\n",
    "Notre jeu de données de **Test** contient **22819 images**. <br />\n",
    "Cependant, dans l'exécution en mode **local**, <br />\n",
    "nous <u>traiterons un ensemble réduit de **393 images (=> 3 images par catégorie)**</u>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69c1767c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get the images' names, paths and features within a spark dataframe.\n",
    "sdf = images.select(\"path\",\n",
    "                    \"label\",\n",
    "                    \"content\",\n",
    "                    preproc_udf(\"content\").alias(\"preprocessed\"),\n",
    "                    featurize_udf(\"content\").alias(\"features\")\n",
    "                   )\n",
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80ffe15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the pySpark df in cache to not restart each step of its making for each further action.\n",
    "sdf.persist()\n",
    "#sdf.persist(StorageLevel.MEMORY_ONLY) # [Default] Cache only in RAM and will recalculate partitions unstored in memory if this one is already full.\n",
    "#sdf.persist(StorageLevel(False, True, False, False, 1))#.MEMORY_ONLY_SER) # Cache only in RAM the serialized (=> Less space used but add the serialize and deserialize steps).\n",
    "#sdf.persist(StorageLevel.MEMORY_AND_DISK) # Once the RAM full, cache the last partitions on disk (=> Slower accessible memory)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4a81cc",
   "metadata": {},
   "source": [
    "*NB : Je ne sais pour quelle raison mais les attributs \"MEMORY_...\" de \"pyspark.StorageLevel\" ne sont pas reconnus. </br> \n",
    "Voici donc une table de leurs correspondances : https://www.tutorialspoint.com/pyspark/pyspark_storagelevel.htm*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919c5134",
   "metadata": {},
   "source": [
    "### 3.8.5 Ajout d'une ACP pour optimiser les performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d37fe50",
   "metadata": {},
   "source": [
    "#### 3.8.5.1 Chargement des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7345f92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow to convert features column's items to VectorUDT type managable by the PCA function of pySpark.\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Import the scaler.\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Import the PCA function of pySpark.\n",
    "from pyspark.ml.feature import PCA\n",
    "#from pyspark.sql import SparkSession\n",
    "\n",
    "# Enable the Apache Arrow to convert pandas dataframes to pySpark dataframes.\n",
    "# NB: Otherwise expect the following error: \"TypeError: Can not infer for type...\".\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "#spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4351fc53",
   "metadata": {},
   "source": [
    "#### 3.8.5.2 Préparer les données à leurs manipulation par pySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "551ad580",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Convert the features column's items from the Array type to VectorUDT type managable by the StandardScaler and the PCA function of pySpark.\n",
    "\n",
    "# Create the UDF converter.\n",
    "list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "\n",
    "# Convert.\n",
    "sdf = sdf.withColumn(\"features\", list_to_vector_udf(sdf[\"features\"]))\n",
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0fd94622",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Normalization of data before its use by the PCA.\n",
    "\n",
    "# Initialize the scaler.\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"norm_features\")\n",
    "\n",
    "# Scale the images' vectors of features.\n",
    "sdf = scaler.fit(sdf).transform(sdf) #.coalesce(1)\n",
    "\n",
    "# Check the resulting table.\n",
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d858c43",
   "metadata": {},
   "source": [
    "#### 3.8.5.3 Détermination du nombre minimal de composantes requises pour expliquer plus de 95 % de la variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c08ef1ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set the number of PCA components over project each feature.\n",
    "# NB: In order to ensure to be able to get at least 95 % of the cumulated variance,\n",
    "#     the number of PCA component is set equal to the vectors' size descibing all image features.\n",
    "tot_n_feat = 1280\n",
    "\n",
    "# Initialize the PCA.\n",
    "pca = PCA(k=tot_n_feat, inputCol=\"norm_features\", outputCol=\"pca_components\")\n",
    "\n",
    "# Generate the PCA model.\n",
    "pca_model = pca.fit(sdf.select(\"norm_features\"))\n",
    "pca_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe5fa3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance of the first principal components:\n",
      " [2.63965484 1.75199614 1.70223333 1.66780977 1.58519688 1.57695503\n",
      " 1.49420937 1.4817065  1.47718064 1.39764534 1.37930265 1.36853959\n",
      " 1.32403286 1.30086333 1.28850203 1.28499589 1.25052706 1.23766956\n",
      " 1.20790266 1.1785739  1.15540918 1.13333598 1.10597205 1.09043724\n",
      " 1.06842348]\n"
     ]
    }
   ],
   "source": [
    "# Get the ratio of explained variance over each PCA components.\n",
    "expl_vars_ratios = pca_model.explainedVariance * 100\n",
    "\n",
    "# Show results.\n",
    "show_sp = 25\n",
    "print(\"Explained variance of the first principal components:\\n\", expl_vars_ratios[:show_sp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d21c9285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   --- Result ---\n",
      "Minimal number of components to reach 95.00 % of cumulated explained variance: 219\n",
      "\n",
      "   --- Check ---\n",
      "Cumulated explained variance over 219 components: 95.06 %\n",
      "Cumulated explained variance over 218 components: 94.99 %\n"
     ]
    }
   ],
   "source": [
    "# Get the cumulated sum of the explained variance ratios.\n",
    "cum_sum_vars_ratios = expl_vars_ratios.cumsum()\n",
    "\n",
    "# Set the minimal cumulated explained variance threshold.\n",
    "cum_var_ratio_limit = 95\n",
    "\n",
    "# Get the minimal number of components to reach 95 % of the explained variance.\n",
    "valid_idx = np.where(cum_sum_vars_ratios >= cum_var_ratio_limit)[0]\n",
    "MIN_N_COMP = valid_idx[cum_sum_vars_ratios[valid_idx].argmin()] + 1\n",
    "\n",
    "# Check the results.\n",
    "print(\"   --- Result ---\")\n",
    "print(\"Minimal number of components to reach %.2f %% of cumulated explained variance: %i\" % (cum_var_ratio_limit, MIN_N_COMP))\n",
    "print()\n",
    "print(\"   --- Check ---\")\n",
    "print(\"Cumulated explained variance over %i components: %.2f \" % (MIN_N_COMP, expl_vars_ratios[:MIN_N_COMP].sum()) + \"%\")\n",
    "print(\"Cumulated explained variance over %i components: %.2f \" % (MIN_N_COMP-1, expl_vars_ratios[:MIN_N_COMP-1].sum()) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09669d6",
   "metadata": {},
   "source": [
    "#### 3.8.5.4 Application de l'ACP sur le nombre de composants principaux déterminés précédemment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "438a46c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.functions import vector_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "31ca9ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def trunc_array_udf (serie_batches):\n",
    "    \n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----------\n",
    "    \n",
    "    This method is a Scalar Iterator pandas UDF wrapping the function which will truncate the vectors gotten after the PCA projection.\n",
    "    \n",
    "    NB: The decorator allows the user to create a method that can be applied over a dataframe with Spark and\n",
    "    specifies the type of the object returned: here a Spark DataFrame column of type ArrayType(FloatType).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    serie: \n",
    "        It is a pandas.Series object corresponding to one column of the spark dataframe.\n",
    "                              \n",
    "    \"\"\"       \n",
    "    \n",
    "    # Loop over each batch of numpy arrays.\n",
    "    for serie_batch in serie_batches:\n",
    "        \n",
    "        # It will return each processed batch before starting the next one.\n",
    "        yield pd.Series([v[:MIN_N_COMP] for v in serie_batch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f3e1f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features projected over the set number of PCA first components.\n",
    "sdf = pca_model.transform(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08494c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/06 20:07:09 WARN DAGScheduler: Broadcasting large task binary with size 12.6 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                path|         label|             content|        preprocessed|            features|       norm_features|      pca_components|\n",
      "+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|file:/media/sf_P8...|    Watermelon|[FF D8 FF E0 00 1...|[1.0, 1.0, 1.0, 1...|[0.05467196926474...|[0.09846937507835...|[-10.467754, -1.8...|\n",
      "|file:/media/sf_P8...|    Watermelon|[FF D8 FF E0 00 1...|[1.0, 1.0, 1.0, 1...|[0.41429647803306...|[0.74618704681238...|[-14.286952, -2.6...|\n",
      "|file:/media/sf_P8...|Pineapple Mini|[FF D8 FF E0 00 1...|[1.0, 1.0, 1.0, 1...|[0.0,4.8250250816...|[0.0,6.8572672519...|[-14.134756, 1.65...|\n",
      "|file:/media/sf_P8...|    Watermelon|[FF D8 FF E0 00 1...|[1.0, 1.0, 1.0, 1...|[0.02374225668609...|[0.04276204443099...|[-13.532433, 1.69...|\n",
      "|file:/media/sf_P8...| Cucumber Ripe|[FF D8 FF E0 00 1...|[1.0, 1.0, 1.0, 1...|[2.12960624694824...|[3.83562178425444...|[-12.302298, 1.40...|\n",
      "+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reconvert the PCA dense vectors objects back to their numpy array counter part then optimize their length.\n",
    "# NB: It was impossible to find a way to directly truncate the PCA projection vectors gotten as dense vector.\n",
    "sdf = sdf.withColumn(\"pca_components\", vector_to_array(\"pca_components\"))\\\n",
    "         .withColumn(\"pca_components\", trunc_array_udf(\"pca_components\"))\n",
    "\n",
    "# Check the resulting table.\n",
    "sdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8814b4",
   "metadata": {},
   "source": [
    "### 3.8.6  Exportation des features au format parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5e83ec",
   "metadata": {},
   "source": [
    "<u>Rappel du PATH où seront inscrits les fichiers au format \"**parquet**\" <br />\n",
    "contenant nos résultats, à savoir, un DataFrame contenant 3 colonnes</u> :\n",
    " 1. Path des images\n",
    " 2. Label de l'image\n",
    " 3. Vecteur de caractéristiques de l'image\n",
    "\n",
    "*NB : Format parquet: https://www.databricks.com/fr/glossary/what-is-parquet*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67fcdb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/sf_P8/GitRepo/datasets/local_results\n"
     ]
    }
   ],
   "source": [
    "print(RESULT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8901db3",
   "metadata": {},
   "source": [
    "<u>Enregistrement des données traitées au format \"**parquet**\"</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "95d07466",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/06 20:07:12 WARN DAGScheduler: Broadcasting large task binary with size 12.8 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Save the dataframe in parquet files.\n",
    "sdf.write.mode(\"overwrite\").parquet(RESULT_PATH) #.repartition(50)\n",
    "#sdf.write.format('parquet').mode(\"overwrite\").save(RESULT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32a1696a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, label: string, content: binary, preprocessed: array<float>, features: vector, norm_features: vector, pca_components: array<float>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the spark dataframe from cache.\n",
    "sdf.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9506f21",
   "metadata": {},
   "source": [
    "## 3.9 Chargement des données enregistrées et validation du résultat\n",
    "\n",
    "<u>On charge les données fraichement enregistrées dans un **DataFrame Pandas**</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "775f0cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load the spark datframe stored in the saves parquet files and convert it to a pandas dataframe..\n",
    "df = spark.read.parquet(RESULT_PATH, engine='pyarrow').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f15070",
   "metadata": {},
   "source": [
    "<u>On affiche les 5 premières lignes du DataFrame</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a1bcdeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "      <th>content</th>\n",
       "      <th>preprocessed</th>\n",
       "      <th>features</th>\n",
       "      <th>norm_features</th>\n",
       "      <th>pca_components</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file:/media/sf_P8/GitRepo/datasets/test_local_...</td>\n",
       "      <td>Watermelon</td>\n",
       "      <td>[255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>[0.05467196926474571, 0.44124868512153625, 0.0...</td>\n",
       "      <td>[0.09846937507835211, 0.6270972911514503, 0.0,...</td>\n",
       "      <td>[-10.467754364013672, -1.8674664497375488, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file:/media/sf_P8/GitRepo/datasets/test_local_...</td>\n",
       "      <td>Watermelon</td>\n",
       "      <td>[255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>[0.4142964780330658, 0.3987191617488861, 0.068...</td>\n",
       "      <td>[0.7461870468123876, 0.5666548472411523, 0.215...</td>\n",
       "      <td>[-14.286952018737793, -2.664612293243408, 1.66...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file:/media/sf_P8/GitRepo/datasets/test_local_...</td>\n",
       "      <td>Pineapple Mini</td>\n",
       "      <td>[255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>[0.0, 4.8250250816345215, 0.007613026537001133...</td>\n",
       "      <td>[0.0, 6.857267251906728, 0.024123965238121135,...</td>\n",
       "      <td>[-14.134756088256836, 1.6541684865951538, 2.71...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file:/media/sf_P8/GitRepo/datasets/test_local_...</td>\n",
       "      <td>Watermelon</td>\n",
       "      <td>[255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>[0.023742256686091423, 0.5131232142448425, 0.0...</td>\n",
       "      <td>[0.042762044430998666, 0.7292445020912333, 0.0...</td>\n",
       "      <td>[-13.532432556152344, 1.6904881000518799, 2.30...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file:/media/sf_P8/GitRepo/datasets/test_local_...</td>\n",
       "      <td>Cucumber Ripe</td>\n",
       "      <td>[255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...</td>\n",
       "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
       "      <td>[2.129606246948242, 0.01786452904343605, 0.0, ...</td>\n",
       "      <td>[3.835621784254447, 0.02538885247385948, 0.0, ...</td>\n",
       "      <td>[-12.302297592163086, 1.4032325744628906, -2.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path           label  \\\n",
       "0  file:/media/sf_P8/GitRepo/datasets/test_local_...      Watermelon   \n",
       "1  file:/media/sf_P8/GitRepo/datasets/test_local_...      Watermelon   \n",
       "2  file:/media/sf_P8/GitRepo/datasets/test_local_...  Pineapple Mini   \n",
       "3  file:/media/sf_P8/GitRepo/datasets/test_local_...      Watermelon   \n",
       "4  file:/media/sf_P8/GitRepo/datasets/test_local_...   Cucumber Ripe   \n",
       "\n",
       "                                             content  \\\n",
       "0  [255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...   \n",
       "1  [255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...   \n",
       "2  [255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...   \n",
       "3  [255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...   \n",
       "4  [255, 216, 255, 224, 0, 16, 74, 70, 73, 70, 0,...   \n",
       "\n",
       "                                        preprocessed  \\\n",
       "0  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n",
       "1  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n",
       "2  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n",
       "3  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n",
       "4  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...   \n",
       "\n",
       "                                            features  \\\n",
       "0  [0.05467196926474571, 0.44124868512153625, 0.0...   \n",
       "1  [0.4142964780330658, 0.3987191617488861, 0.068...   \n",
       "2  [0.0, 4.8250250816345215, 0.007613026537001133...   \n",
       "3  [0.023742256686091423, 0.5131232142448425, 0.0...   \n",
       "4  [2.129606246948242, 0.01786452904343605, 0.0, ...   \n",
       "\n",
       "                                       norm_features  \\\n",
       "0  [0.09846937507835211, 0.6270972911514503, 0.0,...   \n",
       "1  [0.7461870468123876, 0.5666548472411523, 0.215...   \n",
       "2  [0.0, 6.857267251906728, 0.024123965238121135,...   \n",
       "3  [0.042762044430998666, 0.7292445020912333, 0.0...   \n",
       "4  [3.835621784254447, 0.02538885247385948, 0.0, ...   \n",
       "\n",
       "                                      pca_components  \n",
       "0  [-10.467754364013672, -1.8674664497375488, -0....  \n",
       "1  [-14.286952018737793, -2.664612293243408, 1.66...  \n",
       "2  [-14.134756088256836, 1.6541684865951538, 2.71...  \n",
       "3  [-13.532432556152344, 1.6904881000518799, 2.30...  \n",
       "4  [-12.302297592163086, 1.4032325744628906, -2.1...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe dimension: (393, 7)\n"
     ]
    }
   ],
   "source": [
    "# Check the df.\n",
    "display(df.head())\n",
    "print(\"Dataframe dimension:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2794fca",
   "metadata": {},
   "source": [
    "<u>On valide que la dimension du vecteur de caractéristiques des images est bien de dimension 1280</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0bb933b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of the features' vectors: 1280\n",
      "Dimension of the PCA projection of the features' vectors: 219\n"
     ]
    }
   ],
   "source": [
    "# Get the dimension of the features before and after PCA.\n",
    "print(\"Dimension of the features' vectors:\", df.loc[0,'features'].shape[0])\n",
    "print(\"Dimension of the PCA projection of the features' vectors:\", len(df.loc[0,'pca_components']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe5348d",
   "metadata": {},
   "source": [
    "Nous venons de valider le processus sur un jeu de données allégé en local <br />\n",
    "où nous avons simulé un cluster de machines en répartissant la charge de travail <br />\n",
    "sur différents cœurs de processeur au sein d'une même machine.\n",
    "\n",
    "Nous allons maintenant généraliser le processus en déployant notre solution <br />\n",
    "sur un réel cluster de machines et nous travaillerons désormais sur la totalité <br />\n",
    "des 22819 images de notre dossier \"Test\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2690c9f",
   "metadata": {},
   "source": [
    "## 3.10 Exportation du dataframe des transformations des images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "944f1d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the main transformations of the master dataset's dataframe (CSV).\n",
    "#df[[\"path\", \"label\", \"content\", \"features\", \"pca_features\"]].to_csv(os.path.join(RESULT_PATH, \"master_dataset.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d71f4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the main transformations of the master dataset's dataframe (Pickle).\n",
    "# NB: Pickle format was chosen since the file generated is twice lighter on disk and it stores list or arrays better than csv which\n",
    "#     turns into strings all elements and often truncate long once loosing data.\n",
    "df[[\"path\", \"label\", \"content\", \"features\", \"pca_components\"]].to_pickle(os.path.join(RESULT_PATH, \"master_dataset.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eba46f9",
   "metadata": {},
   "source": [
    "# 4. Conclusion\n",
    "\n",
    "**Dans un premier temps la solution a été développé en local**, en tenant compte des contraintes imposés par le projet <br />\n",
    "sur une machine virtuelle dans un environnement Linux Ubuntu.\n",
    "\n",
    "La <u>première phase</u> a consisté à **installer l'environnement de travail Spark**. <br />\n",
    "**Spark** a un paramètre qui nous permet de travaillé en local et nous permet <br />\n",
    "ainsi de **simuler du calcul partagé** en considérant <br />\n",
    "**chaque cœur d'un processeur comme un worker indépendant**.<br />\n",
    "Nous avons travaillé sur un plus **petit jeu de donnée**, l'idée était <br />\n",
    "simplement de **valider le bon fonctionnement de la solution**.\n",
    "\n",
    "Nous avons fait le choix de réaliser du **transfert learning** <br />\n",
    "à partir du model **MobileNetV2**.<br />\n",
    "Ce modèle a été retenu pour sa **légèreté** et sa **rapidité d'exécution** <br />\n",
    "ainsi que pour la **faible dimension de son vecteur en sortie**.\n",
    "\n",
    "Les résultats ont été enregistrés sur disque en plusieurs <br />\n",
    "partitions au format \"**parquet**\".\n",
    "\n",
    "**=> <u>La solution a parfaitement fonctionnée en mode local**</u>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "432.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
